# Default values for sylva-units.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# generic helm chart release name overrides
nameOverride: ""
fullnameOverride: ""

# registry secrets
registry_secret: {}
  # when using gitlab, git creds can be used as registry creds
  #registry.gitlab.com:
  #  username: your_user_name
  #  password: glpat-XXXXX

git_repo_spec_default:
  interval: 60m0s

oci_repo_spec_default:
  interval: 60m0s

source_templates: # template to generate Flux GitRepository/OCIRepository resources
  # <repo-name>:
  #   kind:  GitRepository/OCIRepository
  #   spec:  # partial spec for a Flux resource
  #     url: https://gitlab.com/sylva-projects/sylva-core.git
  #     #secretRef: # is autogenerated based on 'auth'
  #     ref: # can be overridden per-unit, with 'ref_override'
  #       branch: main
  #   auth: # optional 'username'/'password' dict containing git authentication information
  #   existing_source: # optional, when this value is set the specified GitRepository or OCIRepository will be used instead of creating one based on 'spec'
  #     name: sylva-core
  #     kind: GitRepository or OCIRepository

  sylva-core:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-core.git
      ref:
        # this is provided only for reference
        # in practice the sylva-core framework will always override this ref so that the
        # currently checked out commit of sylva-core is used by sylva-units
        # (you can grep the code for "CURRENT_COMMIT" to find out why)
        branch: main

  capi-rancher-import:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/capi-rancher-import.git
      ref:
        tag: 0.1.5

  weave-gitops:
    kind: GitRepository
    spec:
      url: https://github.com/weaveworks/weave-gitops.git
      ref:
        tag: v0.38.0

  metal3:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3.git
      ref:
        tag: 0.7.3

  sylva-capi-cluster:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sylva-capi-cluster.git
      ref:
        tag: 0.1.45

  local-path-provisioner:
    kind: GitRepository
    spec:
      url: https://github.com/rancher/local-path-provisioner.git
      ref:
        tag: v0.0.24

  sriov-resources:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sriov-resources.git
      ref:
        tag: 0.0.3

  os-image-server:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/os-image-server.git
      ref:
        tag: 1.8.0

  capo-contrail-bgpaas:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/capo-contrail-bgpaas.git
      ref:
        tag: 1.0.3

  libvirt-metal:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/container-images/libvirt-metal.git
      ref:
        tag: 0.1.8

  vault-operator:
    kind: GitRepository
    spec:
      url: https://github.com/bank-vaults/vault-operator.git
      ref:
        tag: v1.21.2

  cloud-provider-vsphere:
    kind: GitRepository
    spec:
      url: https://github.com/kubernetes/cloud-provider-vsphere.git
      ref:
        tag: v1.26.2

  sylva-dashboards:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sylva-dashboards.git
      ref:
        tag: 0.0.2

  minio-operator:
    kind: GitRepository
    spec:
      url: https://github.com/minio/operator.git
      ref:
        tag: v5.0.12

  sylva-snmp-resources:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sylva-snmp-resources.git
      ref:
        tag: 0.0.3

  loki:
    kind: GitRepository
    spec:
      url: https://github.com/grafana/loki.git
      ref:
        tag: v2.9.3

  sylva-prometheus-rules:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sylva-prometheus-rules.git
      ref:
        tag: 0.0.7

helm_repo_spec_default:
  interval: 60m0s


# this defines the default .spec for a Kustomization resource
# generated for each item of 'units'
unit_kustomization_spec_default: # default .spec for a Kustomization
  force: false
  prune: true
  interval: 15m
  retryInterval: 1m
  timeout: 30s

# this defines the default .spec for a HelmRelease resource
# generated a unit with a "helmrelease_spec" field
unit_helmrelease_spec_default:  # default for the .spec of a HelmRelease
  interval: 15m
  install:
    remediation:
      retries: 2
      remediateLastFailure: false

# this defines the default .spec for a Kustomization resource containing the HelmRelease resource
# generated by a unit with a "helmrelease_spec" field
unit_helmrelease_kustomization_spec_default:
  path: ./kustomize-units/helmrelease-generic
  sourceRef: >-
    {{- $existing_source := get (index .Values.source_templates "sylva-core") "existing_source" -}}
    {{- if $existing_source -}}
      {{- $existing_source | include "preserve-type" -}}
    {{- else -}}
      {{- dict "kind" (index .Values.source_templates "sylva-core" | dig "kind" "GitRepository") "name" "sylva-core" | include "preserve-type" -}}
    {{- end -}}
  wait: true

# default value used if units.xxx.enabled is not specified
units_enabled_default: false

# unit_template define unit settings
#   a unit can inherit from multiple of those
unit_templates:
  sylva-units:
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/

  # dummy unit template is used to have a Kustomization
  # to add dependencies
  dummy:
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/dummy/base
      wait: false
      postBuild:
        substitute:
          UNIT_NAME: '{{ .Values._unit_name_ }}'

  # this unit template gather depends_on common
  # to most units
  base-deps: {}
    # the actual dependencies aren't the same for bootstrap/management and workload-cluster
    # se the actual content is found in values files specific to each

# this defines Flux HelmRelease objects, and for each  # FIXME
# a corresponding GitRepository or OCIRepository (and Secret, TODO: the Secret don't need to be generated for each unit)
units:
  # <unit-name>:
  #   info: # unit metadata mainly for documentation purpose
  #     description: <short unit description>
  #     details: <more detailed data about unit purpose and usage>
  #     maturity: <level of integration in sylva stack of corresponding component>
  #     internal: true <for units fully defined in sylva-core/kustomize-units without relying on external resources>
  #     version: <force declarative version, not recommended>
  #   enabled: boolean or GoTPL
  #   repo: <name of the repo under 'source_templates'> (for use with kustomization_spec)
  #   helm_repo_url: URL of the Helm repo to use (for use with helmrelease_spec, but not mandatory, 'repo' can be used as well to use a git repo)
  #   labels: (optional) dict holding labels to add to the resources for this unit
  #   ref_override: optional, if defined, this dict will be used for the GitRepository or OCIRepository overriding spec.ref (not used if some helm_repo_* is set)
  #   depends_on: dict defining the dependencies of this unit, keys are unit names, values are booleans
  #               (these dependencies are injected in the unit Kustomization via 'spec.dependsOn')
  #   helmrelease_spec:  # optionnal, contains a partial spec for a FluxCD HelmRelease, all the
  #                      # key things are generated from unit_helmrelease_spec_default
  #                      # and from other fields in the unit definition
  #     _postRenderers: # this field can be used in this file, it will be merged into user-provided 'postRenderers'
  #   helm_chart_artifact_name: optional, if specified, when deploying the Helm chart from an OCI artifact,
  #                             helm_chart_artifact_name will be used as chart name instead of helmrelease_spec.chart.spec.chart last path item
  #                             this is required if helmrelease_spec.chart.spec.chart is empty, '.' or '/'
  #                             (also used by tools/oci/push-helm-chart to generate the artifact)
  #   helm_chart_versions: optional, if specified, when deploying the Helm chart from an OCI artifact or Helm registry,
  #                             it will drive the version to be used from a dict of <version>:<boolean>
  #                             in case if helmrelease_spec.chart.spec.chart.version is not set
  #                             (also used by tools/oci/push-helm-charts-artifacts.sh to generate the artifact)
  #   kustomization_spec:  # contains a partial spec for a FluxCD Kustomization, most of the
  #                        # things are generated from unit_kustomization_spec_default
  #     # sourceRef is generated from .git_repo field
  #     path: ./path-to-unit-under-repo
  #     # the final path will hence be:
  #     # - <git repo template>.spec.url + <unit>.spec.path  (if <git repo template> has spec.url defined)
  #     _patches: # this field can be used in this file, it will be merged into user-provided 'patches'
  #     _components: # this field can be used in this file, it will be merged into user-provided 'components'
  #
  #   helm_secret_values: # (dict), if set what is put here is injected in HelmRelease.valuesFrom as a Secret
  #   kustomization_substitute_secrets: # (dict), if set what is put here is injected in Kustomization.postBuild.substituteFrom as a Secret
  #   unit_templates: optional, list of names of "unit templates"
  #                   unit templates are defined under "unit_templates"
  #                   the settings for the unit are inherited from the corresponding entries under .Values.unit_templates,
  #                   merging them in the specified order

  flux-system:
    info:
      description: contains Flux definitions *to manage the Flux system itself via gitops*
      details: Note that Flux is always installed on the current cluster as a pre-requisite to installing the chart
      maturity: core-component
    repo: sylva-core
    unit_templates: []  # we intendedly don't inherit from base-deps, because flux is itself part of base dependencies
    kustomization_spec:
      path: ./kustomize-units/flux-system/base
      targetNamespace: flux-system
      wait: true
      # prevent Flux from uninstalling itself
      prune: false
      _components:
      - '{{ tuple "../components/extra-ca" .Values.oci_registry_extra_ca_certs | include "set-only-if" }}'
      postBuild:
        substitute:
          var_substitution_enabled: "true" # To force substitution when configmap does not exist
          EXTRA_CA_CERTS: '{{ tuple (.Values.oci_registry_extra_ca_certs | default "" | b64enc) .Values.oci_registry_extra_ca_certs | include "set-only-if" }}'
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
          optional: true

  cert-manager:
    info:
      description: installs cert-manager, an X.509 certificate controller
      maturity: core-component
    unit_templates:
    - base-deps
    helm_repo_url: https://charts.jetstack.io
    helmrelease_spec:
      chart:
        spec:
          chart: cert-manager
          version: v1.13.3
      targetNamespace: cert-manager
      install:
        createNamespace: true
      values:
        installCRDs: true
        replicaCount: >-
          {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 2 1 }}
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: cert-manager
                    app.kubernetes.io/component: controller
                topologyKey: kubernetes.io/hostname
        podDisruptionBudget:
          enabled: true
          minAvailable: >-
            {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 1 0 }}
        webhook:
          replicaCount: >-
            {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 3 1 }}
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app.kubernetes.io/instance: cert-manager
                      app.kubernetes.io/component: webhook
                  topologyKey: kubernetes.io/hostname
          podDisruptionBudget:
            enabled: true
            minAvailable: >-
              {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 1 0 }}
        cainjector:
          replicaCount: >-
            {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 2 1 }}
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app.kubernetes.io/instance: cert-manager
                      app.kubernetes.io/component: cainjector
                  topologyKey: kubernetes.io/hostname
          podDisruptionBudget:
            enabled: true
            minAvailable: >-
              {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 1 0 }}

  trivy-operator:
    info:
      description: installs Trivy operator
      maturity: beta
    enabled: no
    unit_templates:
    - base-deps
    helm_repo_url: https://aquasecurity.github.io/helm-charts/
    helmrelease_spec:
      chart:
        spec:
          chart: trivy-operator
          version: 0.20.0
      targetNamespace: trivy-system
      install:
        createNamespace: true
      values:
        securityContext:
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          readOnlyRootFilesystem: true
          runAsGroup: 10000
          runAsUser: 10000
        serviceAccount:
          annotations: {}
          create: true
          name: trivy-operator
        trivy:
          httpProxy: '{{ .Values.proxies.https_proxy }}'
          httpsProxy: '{{ .Values.proxies.https_proxy }}'
          noProxy: '{{ include "sylva-units.no_proxy" (tuple .) }}'
          severity: UNKNOWN,HIGH,CRITICAL
        trivyOperator:
          scanJobPodTemplatePodSecurityContext:
            runAsGroup: 10000
            runAsUser: 10000

  sylva-ca:
    info:
      description: provides a Certificate Authority for units of the Sylva stack
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
      external-secrets-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca
      wait: true
      postBuild:
        substitute:
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'


  namespace-defs:
    info:
      description: creates sylva-system namespace and other namespaces to be used by various units
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      kyverno-policies: '{{ tuple . "kyverno-policies" | include "unit-enabled" }}' # make sure that the policy disable-automountserviceaccounttoken applies
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/namespace-defs
      wait: true
      prune: false
      _components:
        - '{{ tuple "components/cinder-csi" (tuple . "cinder-csi" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/metal3" (tuple . "metal3" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/longhorn" (tuple . "longhorn" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/keycloak" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/harbor" (tuple . "harbor" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/vault" (tuple . "vault" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/rancher" (tuple . "rancher" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/sriov" (tuple . "sriov" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/cattle-monitoring" (tuple . "monitoring" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/ceph-csi-cephfs" (tuple . "ceph-csi-cephfs" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/cattle-logging" (tuple . "logging" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/gitea" (tuple . "gitea" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/minio-operator" (tuple . "minio-operator" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/minio-monitoring-tenant" (tuple . "minio-monitoring-tenant" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/thanos" (tuple . "thanos" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/loki" (tuple . "loki" | include "unit-enabled") | include "set-only-if" }}'

# Postgresql deployment for keycloak
  postgres-init:
    info:
      description: initializes Postgresql for Keycloak
      internal: true
    enabled_conditions:
    - '{{ tuple . "postgres" | include "unit-enabled"  }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/postgres-init
      wait: true
      postBuild:
        substitute:
          MAX_POD_UNAVAILABLE: '{{ int .Values.cluster.control_plane_replicas | gt 3 | ternary 0 1 }}'

  postgres:
    info:
      description: installs Postgresql for Keycloak
      maturity: stable
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      postgres-init: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: postgresql
          version: 14.0.5
      targetNamespace: keycloak
      values:
        global:
          postgresql:
            auth:
              database: keycloak
        architecture: replication
        nameOverride: postgres
        serviceAccount:
          create: true
          name: postgresql
        readReplicas:
          replicaCount: '{{ int .Values.cluster.control_plane_replicas | gt 3 | ternary 1 3 }}'
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchLabels:
                      app.kubernetes.io/name: postgres
                      app.kubernetes.io/component: read
                  topologyKey: kubernetes.io/hostname

  kubevirt:
    info:
      description: installs kubevirt
      maturity: beta
    unit_templates:
    - base-deps
    helm_repo_url: https://suse-edge.github.io/charts
    helmrelease_spec:
      chart:
        spec:
          chart: kubevirt
          version: 0.2.1
      targetNamespace: kubevirt
      install:
        createNamespace: true
      values:
        kubevirt:
          configuration:
            developerConfiguration:
              featureGates:
                - NUMA
                - CPUManager
                - Snapshot


  kubevirt-test-vms:
    info:
      description: deploys kubevirt VMs for testing
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      kubevirt: true
      multus: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kubevirt-test-vms
      wait: true
      targetNamespace: kubevirt-tests

  harbor-init:
    info:
      description: sets up Harbor prerequisites
      details: it generates namespace, certificate, admin password, OIDC configuration
      internal: true
    depends_on:
      namespace-defs: true
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ tuple . "harbor" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.harbor.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/harbor-init
      wait: true
      postBuild:
        substitute:
          HARBOR_DNS: '{{ .Values.external_hostnames.harbor }}'
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'
          SERVICE: harbor
          SERVICE_DNS: '{{ .Values.external_hostnames.harbor }}'
          CERTIFICATE_NAMESPACE: harbor
          CERT: '{{ .Values.external_certificates.harbor.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.harbor "cert") }}'
        - "../tls-components/sylva-ca"

  harbor:
    info:
      description: installs Harbor
      maturity: beta
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      harbor-init: '{{ tuple . "harbor-init" | include "unit-enabled" }}'
    annotations:
      sylvactl/readyMessage: "Harbor UI can be reached at https://{{ .Values.external_hostnames.harbor }} ({{ .Values.external_hostnames.harbor }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://helm.goharbor.io
    helmrelease_spec:
      chart:
        spec:
          chart: harbor
          version: 1.14.0
      targetNamespace: harbor
      values:
        persistence:
          enabled: true
          resourcePolicy: "keep"
          persistentVolumeClaim:
            registry:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 32Gi
            jobservice:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 8Gi
            database:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 8Gi
            redis:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 8Gi
            #trivy:
              #storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 8Gi
        externalURL: 'https://{{ .Values.external_hostnames.harbor }}'
        existingSecretAdminPassword: '{{ tuple "harbor-init" (tuple . "harbor-init" | include "unit-enabled") | include "set-only-if" }}'
        expose:
          ingress:
            enabled: true
            hosts:
              core: '{{ .Values.external_hostnames.harbor }}'
          tls:
            enabled: true
            certSource: secret
            secret:
              secretName: harbor-tls
        notary:
          enabled: false
        trivy:
          enabled: false
        proxy:
          httpProxy: '{{ get .Values.proxies "http_proxy" }}'
          httpsProxy: '{{ get .Values.proxies "https_proxy" }}'
          noProxy: '{{ include "sylva-units.no_proxy" (tuple .) }}'
      _postRenderers:
        - kustomize:
            patches:
              - target:
                  kind: Deployment|StatefulSet
                patch: |-
                  - op: add
                    path: /spec/template/spec/containers/0/securityContext
                    value:
                      runAsNonRoot: true
                      seccompProfile:
                        type: RuntimeDefault
                      allowPrivilegeEscalation: false
                      capabilities:
                        drop:
                          - ALL
              - target:
                  kind: Deployment
                  name: harbor-registry
                patch: |-
                  - op: add
                    path: /spec/template/spec/containers/1/securityContext
                    value:
                      runAsNonRoot: true
                      seccompProfile:
                        type: RuntimeDefault
                      allowPrivilegeEscalation: false
                      capabilities:
                        drop:
                          - ALL
              - target:
                  kind: StatefulSet
                  name: harbor-database
                patch: |-
                  - op: add
                    path: /spec/template/spec/initContainers/0/securityContext
                    value:
                      runAsNonRoot: true
                      seccompProfile:
                        type: RuntimeDefault
                      allowPrivilegeEscalation: false
                      capabilities:
                        drop:
                          - ALL
                  - op: add
                    path: /spec/template/spec/initContainers/1/securityContext
                    value:
                      runAsNonRoot: true
                      seccompProfile:
                        type: RuntimeDefault
                      allowPrivilegeEscalation: false
                      capabilities:
                        drop:
                          - ALL
      install:
        createNamespace: true

  vault-operator:
    info:
      description: installs Vault operator
      maturity: stable
    depends_on:
      namespace-defs: true
    repo: vault-operator
    unit_templates:
    - base-deps
    helmrelease_spec:
      chart:
        spec:
          chart: deploy/charts/vault-operator
      targetNamespace: vault
      install:
        createNamespace: true
      values:
        image:
          repository: ghcr.io/bank-vaults/vault-operator
          tag: '{{ .Values.source_templates | dig "vault-operator" "spec" "ref" "tag" "" | required "source_templates.vault-operator.spec.ref.tag is unset" }}'

  vault:
    info:
      description: installs Vault
      details: |
        Vault assumes that the certificate vault-tls has been issued
      maturity: stable
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      '{{ .Values._internal.default_storage_class_unit }}': true
      vault-operator: true
    annotations:
      sylvactl/readyMessage: "Vault UI can be reached at https://{{ .Values.external_hostnames.vault }} ({{ .Values.external_hostnames.vault }} must resolve to {{ .Values.display_external_ip }})"
    repo: sylva-core
    kustomization_substitute_secrets:
      ADMIN_PASSWORD: '{{ .Values.admin_password }}'
      KEY: '{{ .Values.external_certificates.vault.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/vault
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.external_hostnames.vault }}'
          VAULT_REPLICAS: '{{ .Values._internal.vault_replicas }}'
          MAX_POD_UNAVAILABLE: '{{ int .Values._internal.vault_replicas | eq 1 | ternary 0 1 }}'
          AFFINITY: '{{ and (eq (int .Values.cluster.control_plane_replicas) 1) (ne .Values._internal.default_storage_class_unit "local-path") | ternary (.Values._internal.vault_no_affinity | toJson | indent 12 ) (.Values._internal.vault_affinity | toJson| indent 12)  }}'
          SERVICE: vault
          SERVICE_DNS: '{{ .Values.external_hostnames.vault }}'
          CERT: '{{ .Values.external_certificates.vault.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        # generate certificate for external communitation
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.vault "cert") }}'
        - "../tls-components/sylva-ca"
      healthChecks:  # sometimes this kustomization seems correctly applied while vault pod is not running, see https://gitlab.com/sylva-projects/sylva-core/-/issues/250
      # so we replace wait:true by checking for the Vault components health
        - apiVersion: apps/v1
          kind: StatefulSet
          name: vault
          namespace: vault
        - apiVersion: apps/v1
          kind: Deployment
          name: vault-operator
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault-operator
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault-configurer
          namespace: vault
        - apiVersion: apps/v1
          kind: Deployment
          name: vault-configurer
          namespace: vault

  vault-config-operator:
    info:
      description: installs Vault config operator
      maturity: stable
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      cert-manager: true
      monitoring: '{{ .Values.units | dig "vault-config-operator" "helmrelease_spec" "values" "enableMonitoring" true }}'
    helm_repo_url: https://redhat-cop.github.io/vault-config-operator
    helmrelease_spec:
      chart:
        spec:
          chart: vault-config-operator
          version: v0.8.25
      targetNamespace: vault
      values:
        enableCertManager: true
        enableMonitoring: false

  vault-secrets:
    info:
      description: generates random secrets in vault, configure password policy, authentication backends, etc...
      internal: true
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      vault: true
      vault-config-operator: true
    kustomization_spec:
      path: ./kustomize-units/vault-secrets
      wait: true
      _components:
        - '{{ tuple "components/keycloak" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'

  vault-oidc:
    info:
      description: configures Vault to be used with OIDC
      internal: true
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      keycloak-resources: true
      vault: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/vault-oidc
      wait: true
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.external_hostnames.vault }}'
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'

  external-secrets-operator:
    info:
      description: installs the External Secrets operator
      maturity: stable
    unit_templates:
    - base-deps
    helm_repo_url: https://charts.external-secrets.io
    helmrelease_spec:
      chart:
        spec:
          chart: external-secrets
          version: 0.9.13
      targetNamespace: external-secrets
      install:
        createNamespace: true
      values:
        installCRDs: true

  eso-secret-stores:
    info:
      description: defines External Secrets stores
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      external-secrets-operator: true
      vault: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/eso-secret-stores
      wait: true
      _components:
        - '{{ tuple "components/keycloak" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'

  cis-operator-crd:
    info:
      description: install CIS operator CRDs
      maturity: stable
      hidden: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "cis-operator" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark-crd
          version: 4.0.0
      targetNamespace: cis-operator-system
      install:
        createNamespace: true

  cis-operator:
    info:
      description: install CIS operator
      maturity: stable
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    depends_on:
      cis-operator-crd: true
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark
          version: 4.0.0
      targetNamespace: cis-operator-system

  cis-operator-scan:
    info:
      description: allows for running a CIS scan for management cluster
      details: |
        it generates a report which can be viewed and downloaded in CSV from the Rancher UI, at https://rancher.sylva/dashboard/c/local/cis/cis.cattle.io.clusterscan
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "cis-operator" | include "unit-enabled" }}'
    depends_on:
      cis-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cis-operator-scan
      wait: false
      postBuild:
        substitute:
          SCAN_PROFILE: '{{ .Values.cis_benchmark_scan_profile }}'

  neuvector-init:
    info:
      description: sets up Neuvector prerequisites
      details: |
        it generates namespace, certificate, admin password, policy exception for using latest tag images (required for the pod managing the database of vulnerabilities since this DB is updated often)
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "neuvector" | include "unit-enabled" }}'
    depends_on:
      sylva-ca: true
      vault: true
      vault-config-operator: true
      kyverno: true
      keycloak-add-client-scope: true
      keycloak-oidc-external-secrets: true
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.neuvector.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/neuvector-init
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: neuvector-init
          namespace: neuvector
        - apiVersion: v1
          kind: Secret
          name: neuvector-oidc-init
          namespace: neuvector
      postBuild:
        substitute:
          NEUVECTOR_DNS: '{{ .Values.external_hostnames.neuvector }}'
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'
          SERVICE: neuvector
          SERVICE_DNS: '{{ .Values.external_hostnames.neuvector }}'
          CERTIFICATE_NAMESPACE: neuvector
          CERT: '{{ .Values.external_certificates.neuvector.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.neuvector "cert") }}'
        - "../tls-components/sylva-ca"

  neuvector:
    info:
      description: installs Neuvector
      maturity: beta
    enabled: no
    unit_templates:
    - base-deps
    depends_on:
      neuvector-init: true
      ingress-nginx: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
    annotations:
      sylvactl/readyMessage: "Neuvector UI can be reached at https://{{ .Values.external_hostnames.neuvector }} ({{ .Values.external_hostnames.neuvector }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://neuvector.github.io/neuvector-helm
    helm_chart_artifact_name: neuvector-core
    helmrelease_spec:
      chart:
        spec:
          chart: core
          version: 2.6.6
      targetNamespace: neuvector
      values:
        leastPrivilege: true
        internal:
          certmanager:
            enabled: true
            secretname: neuvector-internal
        controller:
          replicas: 1  # PVC only works for 1 replica https://github.com/neuvector/neuvector-helm/issues/110#issuecomment-1251921734
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxSurge: 1
              maxUnavailable: 1
          image:
            repository: neuvector/controller
          pvc:
            enabled: true # setting PVC to true imposes 1 replica https://github.com/neuvector/neuvector-helm/issues/110#issuecomment-1251921734
            accessModes:
              - ReadWriteOnce
        enforcer:
          image:
            repository: neuvector/enforcer
        manager:
          image:
            repository: neuvector/manager
          runAsUser: 10000
          ingress:
            enabled: true
            host: '{{ .Values.external_hostnames.neuvector }}'
            ingressClassName: nginx
            path: /
            annotations:
              nginx.ingress.kubernetes.io/backend-protocol: https
            tls: true
            secretName: neuvector-tls
        cve:
          updater:
            image:
              repository: neuvector/updater
          scanner:
            image:
              repository: neuvector/scanner
              env:
                - name: https_proxy
                  value: '{{ .Values.proxies.https_proxy }}'
                - name: no_proxy
                  value: '{{ include "sylva-units.no_proxy" (tuple .) }}'
        containerd:
          enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" | include "as-bool" }}'
          path: /var/run/containerd/containerd.sock
        k3s:
          enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | include "as-bool" }}'
          runtimePath: /run/k3s/containerd/containerd.sock
        resources:
          limits:
            cpu: 400m
            memory: 2792Mi
          requests:
            cpu: 100m
            memory: 2280Mi
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: CronJob
                apiVersion: batch/v1
                metadata:
                  name: neuvector-updater-pod
                  namespace: neuvector
                spec:
                  startingDeadlineSeconds: 21600
    kustomization_spec:
      healthChecks:
        - apiVersion: apps/v1
          kind: Deployment
          name: neuvector-manager-pod
          namespace: neuvector
        - apiVersion: apps/v1
          kind: Deployment
          name: neuvector-controller-pod
          namespace: neuvector

  keycloak:
    info:
      description: initializes and configures Keycloak
      maturity: stable
    unit_templates:
    - base-deps
    depends_on:
      sylva-ca: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      postgres: true
      synchronize-secrets: true # make sure that the secret keycloak-initial-admin is ready to be consummed
    annotations:
      sylvactl/readyMessage: "Keycloak admin console can be reached at https://{{ .Values.external_hostnames.keycloak }}/admin/master/console, user 'admin', password in Vault at secret/keycloak ({{ .Values.external_hostnames.keycloak }} must resolve to {{ .Values.display_external_ip }})"
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.keycloak.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/keycloak
      targetNamespace: keycloak
      postBuild:
        substitute:
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'
          SERVICE: keycloak
          SERVICE_DNS: '{{ .Values.external_hostnames.keycloak }}'
          CERT: '{{ .Values.external_certificates.keycloak.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
      healthChecks:  # cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        # the Keycloak StatefulSet set is produced, by the combination of Keycloak operator
        # and a Keycloak custom resource, it relies on the postgres DB also deployed by this unit
        # hence, checking for the health of this component can be done by checking this StatefulSet
        - apiVersion: apps/v1
          kind: StatefulSet
          name: keycloak
          namespace: keycloak
      _components:
        - '{{ tuple "components/keycloak-operator-proxies" (.Values.proxies.https_proxy) | include "set-only-if" }}'
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.keycloak "cert") }}'
      _patches:
        - patch: |
            - op: replace
              path: /spec/template/spec/containers/0/securityContext
              value:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
                privileged: false
                runAsNonRoot: true
                runAsGroup: 1000
                runAsUser: 1000
                seccompProfile:
                  type: RuntimeDefault
          target:
            kind: Deployment
            name: keycloak-operator

  keycloak-legacy-operator:
    info:
      description: installs Keycloak "legacy" operator
      maturity: stable
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      vault-secrets: true  # the credential-external-keycloak Secret use by the legacy operator is generated from ES/Vault secret/data/keycloak
      keycloak: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-legacy-operator
      targetNamespace: keycloak
      wait: true
      _patches:
        - patch: |
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: keycloak-realm-operator
            spec:
              template:
                spec:
                  containers:
                    - name: keycloak-realm-operator
                      securityContext:
                        runAsUser: 10000
                        allowPrivilegeEscalation: false
                        capabilities:
                          drop:
                            - ALL
                        runAsNonRoot: true
                        seccompProfile:
                          type: RuntimeDefault
          target:
            kind: Deployment
            name: keycloak-realm-operator

  keycloak-resources:
    info:
      description: configures keycloak resources
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
    repo: sylva-core
    kustomization_substitute_secrets:
      SSO_PASSWORD: '{{ .Values.admin_password }}'
    kustomization_spec:
      path: ./kustomize-units/keycloak-resources
      targetNamespace: keycloak
      _components:
        - '{{ tuple "components/neuvector" (tuple . "neuvector" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/harbor" (tuple . "harbor" | include "unit-enabled") | include "set-only-if" }}'
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.external_hostnames.rancher }}'
          FLUX_WEBUI_DNS: '{{ .Values.external_hostnames.flux }}'
          HARBOR_DNS: '{{ .Values.external_hostnames.harbor }}'
          NEUVECTOR_DNS: '{{ tuple .Values.external_hostnames.neuvector (tuple . "neuvector" | include "unit-enabled") | include "set-only-if" }}'
          VAULT_DNS: '{{ .Values.external_hostnames.vault }}'
          EXPIRE_PASSWORD_DAYS: '{{ int .Values.keycloak.keycloak_expire_password_days }}'
      healthChecks:  #  cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        - apiVersion: legacy.k8s.keycloak.org/v1alpha1
          kind: KeycloakRealm
          name: sylva
          namespace: keycloak
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-rancher-client  # this secret is a byproduct of the rancher-client KeycloakClient resource
          namespace: keycloak
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-flux-webui-client  # this secret is a byproduct of the flux-webui-client KeycloakClient resource
          namespace: keycloak
        - '{{ tuple (dict "apiVersion" "v1" "kind" "Secret" "name" "keycloak-client-secret-neuvector-client" "namespace" "keycloak") (tuple . "neuvector" | include "unit-enabled") | include "set-only-if" }}'  # # this secret is a byproduct of the neuvector client KeycloakClient resource
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-harbor-client  # this secret is a byproduct of the harbor-client KeycloakClient resource
          namespace: keycloak


  keycloak-add-client-scope:
    info:
      description: configures Keycloak client-scope
      details: >
        a job to manually add a custom client-scope to sylva realm (on top of default ones)
        while CRD option does not yet provide good results (overrides defaults)
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "flux-webui" | include "unit-enabled" }}'
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      keycloak-resources: true
      keycloak: true  # defines the keycloak-initial-admin Secret used by the script
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      postBuild:
        substitute:
          JOB_NAME: keycloak-add-client-scope-job
          JOB_TARGET_NAMESPACE: keycloak
          RUNASUSER: '10000'
          RUNASGROUP: '10000'
      _patches:
      - target:
          kind: Job
        patch: |
          - op: replace
            path: /spec/backoffLimit
            value: 15
      - target:
          kind: ConfigMap
        patch: |
          - op: replace
            path: /metadata/name
            value: keycloak-add-client-scope-job-keycloak-cm
          - op: replace
            path: /data/kube-job.sh
            value: |
          {{ .Files.Get "scripts/keycloak-add-client-scope.sh" | indent 4 }}

  keycloak-oidc-external-secrets:
    info:
      description: configures OIDC secrets for Keycloack
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
      keycloak-resources: true
      eso-secret-stores: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-oidc-external-secrets
      postBuild:
        substitute:
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'
          FLUX_WEBUI_DNS: '{{ .Values.external_hostnames.flux }}'
      wait: false
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: oidc-auth
          namespace: flux-system

  kyverno:
    info:
      description: installs Kyverno
      maturity: stable
    unit_templates:
    - base-deps
    helm_repo_url: https://kyverno.github.io/kyverno
    helmrelease_spec:
      chart:
        spec:
          chart: kyverno
          version: 3.1.4
      targetNamespace: kyverno
      install:
        createNamespace: true
      values:
        webhooksCleanup:
          enabled: false
        features:
          policyExceptions:
            enabled: true
        admissionController:
          replicas: '{{ .Values._internal.default_replicas | include "preserve-type" }}'
        cleanupController:
          replicas: '{{ .Values._internal.default_replicas | include "preserve-type" }}'
        reportsController:
          replicas: '{{ .Values._internal.default_replicas | include "preserve-type" }}'
        backgroundController:
          replicas: '{{ .Values._internal.default_replicas | include "preserve-type" }}'
          rbac:
            clusterRole:
              extraResources:
              - apiGroups: ["helm.toolkit.fluxcd.io"]
                resources:
                  - "helmreleases"
                verbs: ["get", "list", "watch", "patch", "update"]
              - apiGroups: ["external-secrets.io"]
                resources:
                  - "externalsecrets"
                verbs: ["get", "list", "watch", "patch", "update"]
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: CronJob
                apiVersion: batch/v1
                metadata:
                  name: kyverno-cleanup-cluster-admission-reports
                  namespace: kyverno
                spec:
                  startingDeadlineSeconds: 180
              - kind: CronJob
                apiVersion: batch/v1
                metadata:
                  name: kyverno-cleanup-admission-reports
                  namespace: kyverno
                spec:
                  startingDeadlineSeconds: 180


  kyverno-policies:
    info:
      description: configures Kyverno policies
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "kyverno" | include "unit-enabled" }}'
    depends_on:
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kyverno-policies
      wait: true

  rancher-monitoring-clusterid-inject:
    info:
      description: injects Rancher cluster ID in Helm values of Rancher monitoring chart
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "kyverno" | include "unit-enabled" }}'
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    depends_on:
      kyverno: '{{ tuple . "kyverno" | include "unit-enabled" }}'
      rancher: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/rancher-monitoring-clusterid-inject
      wait: true

  shared-workload-clusters-settings:
    info:
      description: manages parameters which would be shared between management and workload clusters
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/shared-workload-clusters-settings
      wait: true
      targetNamespace: sylva-system
      postBuild:
        substitute:
          OS_IMAGES_INFO_CM: '{{ .Values._internal.os_images_info_configmap }}'
      _patches:
        - target:
            kind: ConfigMap
          patch: |
            - op: replace
              path: /data/values
              value: |
            {{ index (tuple . .Values.shared_workload_clusters_values | include "interpret-inner-gotpl" | fromJson) "result" | toYaml | indent 4 }}
    kustomization_substitute_secrets:
      SECRET_VALUES: '{{ index (tuple . .Values.shared_workload_clusters_secret_values | include "interpret-inner-gotpl" | fromJson) "result" | toYaml | b64enc }}'

  capi:
    info:
      description: installs Cluster API core operator
      maturity: core-component
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capi
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  capd:
    info:
      description: installs Docker CAPI infra provider
      maturity: core-component
      kustomization_path: ./kustomize-units/capd/base
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: '{{ ternary "./kustomize-units/capd/base" "./kustomize-units/capd/rke2" (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") }}'
      postBuild:
        substitute:
          CAPD_DOCKER_HOST: '{{ .Values.capd_docker_host }}'
      wait: true

  capo:
    info:
      description: installs OpenStack CAPI infra provider
      maturity: core-component
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capo
      wait: true
      _patches:
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: capo-controller-manager
            namespace: capo-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL

  capm3:
    info:
      description: installs Metal3 CAPI infra provider, for baremetal
      maturity: core-component
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
      '{{ .Values._internal.metal3_unit }}': true
    kustomization_spec:
      path: ./kustomize-units/capm3
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
          enableBMHNameBasedPreallocation: "true"
      wait: true

  capv:
    info:
      description: installs vSphere CAPI infra provider
      maturity: core-component
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capv
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  cabpk:
    info:
      description: installs Kubeadm CAPI bootstrap provider
      maturity: core-component
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpk
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  cabpr:
    info:
      description: installs RKE2 CAPI bootstrap provider
      maturity: core-component
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpr
      wait: true
      _patches:
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: rke2-bootstrap-controller-manager
            namespace: rke2-bootstrap-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: rke2-control-plane-controller-manager
            namespace: rke2-control-plane-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
  metal3-suse:
    info:
      description: installs SUSE-maintained Metal3 operator
      maturity: beta
    enabled: false
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    helm_repo_url: https://suse-edge.github.io/charts
    helm_chart_artifact_name: metal3-suse
    helmrelease_spec:
      chart:
        spec:
          chart: metal3
          version: 0.6.0
      timeout: 30m
      install:
        createNamespace: true
      targetNamespace: metal3-system
      values:
        global:
          ironicIP: '{{ .Values.cluster_virtual_ip }}'
          provisioningInterface: eth0
        metal3-ironic:
          service:
            type: LoadBalancer
            externalIPs:
            - '{{ .Values.cluster_virtual_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
          baremetaloperator:
            ironichostNetwork: false
          persistence:
            ironic:
              size: 10Gi
              storageClass: '{{ .Values._internal.default_storage_class }}'
              accessMode: ReadWriteOnce
        metal3-mariadb:
          persistence:
            storageClass: '{{ .Values._internal.default_storage_class }}'

  metal3-init:
    info:
      description: generates Metal3 random credentials
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "metal3" | include "unit-enabled" }}'
    depends_on:
      vault: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/metal3-init
      wait: true

  metal3-sync-secrets:
    info:
      description: configures secrets for Metal3 components
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "metal3" | include "unit-enabled" }}'
    depends_on:
      metal3-init: true
      eso-secret-stores: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/metal3-sync-secrets
      wait: true

  metal3:
    info:
      description: install Metal3 operator
      maturity: stable
    enabled: false
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
      '{{ .Values._internal.default_storage_class_unit }}': true
      metal3-sync-secrets: true # make sure that the password secrets are ready to be consumed
    kustomization_spec:
      postBuild:
        substitute:
          # Force substitution in order to be consistent with what is done in bootstrap,
          # See https://gitlab.com/sylva-projects/sylva-core/-/issues/659
          var_substitution_enabled: "true"
    repo: metal3
    helm_chart_artifact_name: metal3-sylva
    helmrelease_spec:
      timeout: 30m
      chart:
        spec:
          chart: .
      install:
        createNamespace: false
      targetNamespace: metal3-system
      values:  # see https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3/-/blob/main/values.yaml
        global:
          storageClass: '{{ .Values._internal.default_storage_class }}'
        # ironicIPADownloaderBaseURI:
        noProxy: '{{ include "sylva-units.no_proxy" (tuple .) }}'
        images:
          baremetalOperator:
            repository: quay.io/metal3-io/baremetal-operator
            tag: "v0.5.0"
          ironic:
            repository: quay.io/metal3-io/ironic
            tag: "capm3-v1.5.2"
          ironicIPADownloader:
            repository: registry.gitlab.com/sylva-projects/sylva-elements/container-images/ironic-ipa-downloader/ipa
            tag: "0.2.0"
        persistence:
          ironic:
            size: "10Gi"
            accessMode: "ReadWriteOnce"
        services:
          ironic:
            # Specify the IP address used by Ironic service
            ironicIP: '{{ .Values.display_external_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
        mariadb:
          persistence:
            storageClass: '{{ .Values._internal.default_storage_class }}'
        podSecurityContext:
          runAsNonRoot: true
          runAsUser: 10000
        securityContext:
          runAsNonRoot: true
          runAsUser: 10000
      valuesFrom:
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-mariadb-root-secret
          targetPath: mariadb.auth.rootPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-mariadb-replication-secret
          targetPath: mariadb.auth.replicationPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-secret
          targetPath: mariadb.auth.ironicPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-secret
          targetPath: auth.ironicPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-inspector-secret
          targetPath: auth.ironicInspectorPassword
          optional: false

  capi-providers-pivot-ready:
    info:
      description: checks if management cluster is ready for pivot
      details: >
        This unit only has dependencies, but does not create resources.
        It is here only to have a single thing to look at to determine
        if everything is ready for pivot (see bootstrap.values.yaml pivot unit)
      internal: true
    unit_templates:
    - base-deps
    - dummy
    # we copy the dependencies of the 'cluster' unit
    # (with the exception of "capo-cluster-resources" which we don't need, given how what it produces is consumed)
    depends_on: '{{ omit .Values.units.cluster.depends_on "capo-cluster-resources" | include "preserve-type" }}'

  local-path-provisioner:
    info:
      description: installs local-path CSI
      maturity: stable
    repo: local-path-provisioner
    unit_templates:
    - base-deps
    helmrelease_spec:
      chart:
        spec:
          chart: deploy/chart/local-path-provisioner
      targetNamespace: kube-system
      install:
        createNamespace: true
      values:
        storageClass:
          defaultClass: '{{ .Values._internal.default_storage_class | eq "local-path" | include "as-bool" }}'
        helperImage:
          repository: docker.io/library/busybox
          tag: 1.36.1
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: local-path-provisioner
                spec:
                  template:
                    spec:
                      containers:
                      - name: local-path-provisioner
                        securityContext:
                          runAsUser: 10000

  cluster:
    info:
      description: holds the Cluster API definition for the cluster
      maturity: core-component
    repo: sylva-capi-cluster
    unit_templates:
    - base-deps
    helm_chart_artifact_name: sylva-capi-cluster
    depends_on:
      capi: true
      '{{ .Values.cluster.capi_providers.infra_provider }}': true
      '{{ .Values.cluster.capi_providers.bootstrap_provider }}': true
      capo-cluster-resources: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
      get-openstack-images: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
      '{{ .Values._internal.metal3_unit }}': '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
      sylva-system/os-image-server: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
    labels:
      suspend-on-pivot: "yes"  # must be suspended before pivot
    kustomization_spec:
      prune: '{{ not (eq .Release.Namespace "sylva-system") | include "preserve-type" }}'
      # we wait on all important resources built by sylva-capi-cluster,
      # *except* the MachineDeployments, since if we're using kubeadm as bootstrap
      # we would have a deadlock: the default CNI unit would not deploy
      #  before the cluster unit is ready, and the cluster would not be ready until
      #  the CNI is deployed because the MachineDeployment nodes need the CNI to become
      #  ready (for the controlplane nodes, the kubeadm controlplane provider ignores that)
      healthChecks: >-
        {{
        (include "cluster-healthchecks" (dict "ns" .Release.Namespace "cluster" .Values.cluster "includeMDs" false) | fromYaml).result
        | include "preserve-type"
        }}
    helmrelease_spec:
      targetNamespace: '{{ .Release.Namespace }}'
      chart:
        spec:
          chart: .
      valuesFrom:
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_ip
          targetPath: cluster_virtual_ip
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_fip
          targetPath: cluster_public_ip
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: control_plane_servergroup_id
          targetPath: control_plane.capo.server_group_id
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: worker_servergroup_id
          targetPath: machine_deployment_default.capo.server_group_id
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: openstack-images-uuids  ## this ConfigMap is a byproduct of the get-openstack-images job
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: '{{ .Values._internal.os_images_info_configmap }}'  # this configmap is a byproduct of the os-images-info unit
          valuesKey: values-s-c-c.yaml
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capm3") | include "as-bool" }}'
    # we pass everything that is under `cluster` to this unit that uses sylva-capi-cluster chart
    # (we do it via a secret because some of the values are credentials in many scenarios)
    helm_secret_values: '{{ .Values.cluster | include "preserve-type" }}'

  cluster-garbage-collector:
    info:
      description: installs cronjob responsible for unused CAPI resources cleaning
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      cluster: false # we can't depend directly on 'cluster' unit, since it's being disabled in 'management-sylva-units' and re-enabled by 'pivot'
      capi: true
      capd: '{{ tuple . "capd" | include "unit-enabled" }}'
      capv: '{{ tuple . "capv" | include "unit-enabled" }}'
      capo: '{{ tuple . "capo" | include "unit-enabled" }}'
      capm3: '{{ tuple . "capm3" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cluster-garbage-collector
      wait: true
      _components:
        - '{{ tuple "components/dev-ci-cronjob-schedule" (list "dev" "ci" | has .Values.env_type) | include "set-only-if" }}'

  cluster-ready:
    info:
      description: unit to check readiness of cluster CAPI objects
      internal: true
    unit_templates:
    - dummy
    enabled_conditions:
    - '{{ tuple . "cluster" | include "unit-enabled" }}'
    depends_on:
      cluster: true
    kustomization_spec:
      healthChecks: >-
        {{
        (include "cluster-healthchecks" (dict "ns" .Release.Namespace "cluster" .Values.cluster "sleep_job" true) | fromYaml).result
        | include "preserve-type"
        }}
      # we add a sleep to overcome a race condition happening when
      # from bootstrap cluster, after pivot, the cluster unit is enabled
      _components:
        - ../components/sleep-job
      postBuild:
        substitute:
          SLEEP_TIME: 60s

  cluster-reachable:
    info:
      internal: true
      description: ensure that created clusters are reachable, and make failure a bit more explicit if it is not the case
      details: >
        This unit will be enabled in bootstrap cluster to check connectivity to management cluster
        and in various workload-cluster namespaces in management cluster to check connectivity to workload clusters
    unit_templates:
    - dummy
    enabled: false
    depends_on:
      cluster: true
    kustomization_spec:
      targetNamespace: default
      kubeConfig:
        secretRef:
          name: '{{ .Values.cluster.name }}-kubeconfig'

  heat-operator:
    info:
      description: installs OpenStack Heat operator
      maturity: core-component
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/heat-operator
      wait: true
      images:
        - name: controller
          newName: registry.gitlab.com/sylva-projects/sylva-elements/heat-operator
          newTag: 0.0.7


  sylva-units-operator:
    info:
      description: installs sylva-units operator
      maturity: experimental
    unit_templates:
    - base-deps
    repo: sylva-core
    depends_on:
      flux-system: true
    kustomization_spec:
      path: ./kustomize-units/sylva-units-operator
      wait: true
      images:
        - name: controller
          newName: registry.gitlab.com/sylva-projects/sylva-elements/sylva-units-operator
          newTag: 0.0.0-pre4

  workload-cluster-operator:
    info:
      description: installs Sylva operator for managing workload clusters
      maturity: experimental
    unit_templates:
    - base-deps
    repo: sylva-core
    depends_on:
      sylva-units-operator: true
    kustomization_spec:
      path: ./kustomize-units/workload-cluster-operator
      wait: true
      images:
        - name: controller
          newName: registry.gitlab.com/sylva-projects/sylva-elements/workload-cluster-operator
          newTag: 0.0.0-pre3

  capo-cluster-resources:
    info:
      description: installs OpenStack Heat stack for CAPO cluster prerequisites
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      heat-operator: true
    kustomization_spec:
      path: ./kustomize-units/capo-cluster-resources
      wait: true
      targetNamespace: '{{ .Release.Namespace }}'
      postBuild:
        substitute:
          STACK_NAME_PREFIX: '{{ .Values.cluster.name }}-{{ tuple . .Values.cluster.capo.resources_tag | include "interpret-as-string" | replace "." "-" }}'
          CAPO_TAG: '{{ .Values.cluster.capo.resources_tag }}'
          CAPO_NETWORK_ID: '{{ .Values.cluster.capo.network_id }}'
          CONTROL_PLANE_AFFINITY_POLICY: '{{ .Values.openstack.control_plane_affinity_policy }}'
          WORKER_AFFINITY_POLICY: '{{ .Values.openstack.worker_affinity_policy }}'
          CAPO_EXTERNAL_NETWORK_ID: '{{ tuple .Values.openstack.external_network_id .Values.openstack.external_network_id | include "set-only-if" }}'
          CONTROL_PLANE_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}'
          WORKER_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}'
          CAPO_CREATE_IRONIC_SECURITY_GROUP: '{{ tuple . (and (tuple . "metal3" | include "unit-enabled") (.Values.cluster.capi_providers.infra_provider | eq "capo")) "true" "false" | include "interpret-ternary" }}'
          COMMON_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-common-{{ .Values.cluster.capo.resources_tag }}'
    kustomization_substitute_secrets:
      CAPO_CLOUD_YAML: '{{ .Values.cluster.capo.clouds_yaml  | toYaml | b64enc }}'
      CAPO_CACERT: '{{ (.Values.cluster.capo.cacert|default "") | b64enc }}'

  calico-crd:
    info:
      description: installs Calico CRDs
      maturity: stable
      hidden: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "calico" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      v3.25.001: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      v3.26.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      releaseName: rke2-calico-crd
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-calico-crd
          version: "" # will be defined by helm_chart_versions

  tigera-clusterrole:
    info:
      description: is here to allow for upgrading Calico chart when upgrading cluster
      details: |
        For v1.25.x to v1.26.x, see https://gitlab.com/sylva-projects/sylva-core/-/issues/664
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "calico" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/tigera-clusterrole
      wait: true
      force: true

  calico:
    info:
      description: install Calico CNI
      maturity: stable
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'  # installed by RKE2 when RKE2 is used
    unit_templates:
    - base-deps
    depends_on:
      calico-crd: true
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      v3.25.001: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      v3.26.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      chart:
        spec:
          chart: rke2-calico
          version: "" # will be defined by helm_chart_versions
      values:
        installation:
          calicoNetwork:
            bgp: Enabled
          registry: UseDefault
        felixConfiguration:
          wireguardEnabled: '{{ .Values.calico_wireguard_enabled | include "preserve-type" }}'

  metallb:
    info:
      description: installs MetalLB operator
      maturity: stable
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    depends_on:
      calico: true
    helm_repo_url: https://metallb.github.io/metallb
    helmrelease_spec:
      chart:
        spec:
          chart: metallb
          version: 0.13.12
      targetNamespace: metallb-system
      install:
        createNamespace: true
      values:
        controller:
          nodeSelector:
            node-role.kubernetes.io/control-plane: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | ternary "true" "" }}'
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
        speaker:
          frr:
            enabled: false
          nodeSelector:
            node-role.kubernetes.io/control-plane: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | ternary "true" "" }}'
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane

  cinder-csi:
    info:
      description: installs OpenStack Cinder CSI
      maturity: stable
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    depends_on:
      namespace-defs: true
    helm_repo_url: https://kubernetes.github.io/cloud-provider-openstack
    helmrelease_spec:
      chart:
        spec:
          chart: openstack-cinder-csi
          version: 2.28.1
      targetNamespace: cinder-csi
      install:
        createNamespace: false
      values:
        clusterID: '{{ .Values.cluster.capo.resources_tag }}'
        storageClass:
          enabled: false
          delete:
            isDefault: false
            allowVolumeExpansion: true
          retain:
            isDefault: false
            allowVolumeExpansion: true
          custom: |-
            ---
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: "{{ .Values.openstack.storageClass.name }}"
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
            provisioner: cinder.csi.openstack.org
            volumeBindingMode: Immediate
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              type: "{{ .Values.openstack.storageClass.type }}"
    helm_secret_values:
      secret:
        enabled: "true"
        create: "true"
        name: cinder-csi-cloud-config
        data:
          cloud.conf: |-
            {{- if .Values.cluster.capi_providers.infra_provider | eq "capo" -}}
            [Global]
            auth-url = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.auth_url | quote }}
            tenant-name = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.project_name | quote }}
            domain-name = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.user_domain_name | quote }}
            username = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username | quote }}
            password = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.password | quote }}
            region = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.region_name | quote }}
            tls-insecure = {{ not .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.verify }}
            [BlockStorage]
            ignore-volume-az = true
            {{- end -}}

  synchronize-secrets:
    info:
      description: allows secrets from Vault to be consumed other units, relies on ExternalSecrets
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      eso-secret-stores: true
      vault-secrets: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/synchronize-secrets
      wait: true
      _components:
        - '{{ tuple "components/keycloak" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'
      postBuild:
        substitute:
          FLUX_ADMIN_USERNAME: '{{ .Values.flux_webui.admin_user }}'

  rancher-init:
    info:
      description: initializes and configures Rancher
      maturity: beta
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      sylva-ca: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.rancher.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/rancher-init
      targetNamespace: cattle-system
      postBuild:
        substitute:
          SERVICE: rancher
          SERVICE_DNS: '{{ .Values.external_hostnames.rancher }}'
          CERT: '{{ .Values.external_certificates.rancher.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.rancher "cert") }}'
        - "../tls-components/sylva-ca"
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: rancher-tls
          namespace: cattle-system

  rancher:
    info:
      description: installs Rancher
      maturity: stable
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      cert-manager: true
      k8s-gateway: true
      rancher-init: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      synchronize-secrets: true
    annotations:
      sylvactl/readyMessage: "Rancher UI can be reached at https://{{ .Values.external_hostnames.rancher }} ({{ .Values.external_hostnames.rancher }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://releases.rancher.com/server-charts/latest
    helmrelease_spec:
      chart:
        spec:
          chart: rancher
          version: 2.8.2
      targetNamespace: cattle-system
      interval: 10m0s
      upgrade:
        remediation:
          retries: 3
      values:
        auditLog:
          level: '{{ .Values.audit_log.level }}'
        privateCA: true
        useBundledSystemChart: true
        hostname: '{{ .Values.external_hostnames.rancher }}'
        ingress:
          enabled: true
          ingressClassName: nginx
          tls:
            source: secret
            secretName: rancher-tls
        # restrictedAdmin: true
        # negative value will deploy 1 to abs(replicas) depending on available number of nodes
        replicas: -3
        features: embedded-cluster-api=false,provisioningv2=true
        debug: true
        proxy: '{{ get .Values.proxies "https_proxy" }}'
        noProxy: '{{ include "sylva-units.no_proxy" (tuple .) }}'
        postDelete:
          namespaceList:
            - cattle-fleet-system
            - rancher-operator-system
        extraEnv:
          - name: CATTLE_BOOTSTRAP_PASSWORD
            valueFrom:
              secretKeyRef:
                name: bootstrap-secret
                key: bootstrapPassword
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: rancher
                spec:
                  template:
                    spec:
                      volumes:
                        - name: tls-ca-volume
                          secret:
                            defaultMode: 256
                            secretName: rancher-tls
                            items:
                              - key: ca.crt
                                path: cacerts.pem
                      # this is to avoid that the too-short default liveness probe
                      # prevents the Rancher installation from finishing before the pod is killed:
                      containers:
                        - name: rancher
                          livenessProbe:
                            initialDelaySeconds: 120
                            periodSeconds: 30
                            failureThreshold: 20

    kustomization_spec:
      # these healthChecks are added so that does not become ready before
      # a few things that Rancher sets up behind the scene are ready
      healthChecks:
        - apiVersion: apiextensions.k8s.io/v1
          kind: CustomResourceDefinition
          name: clusters.provisioning.cattle.io  # this is because capi-rancher-import needs this
        - apiVersion: apps/v1
          kind: Deployment
          name: rancher-webhook
          namespace: cattle-system
        - apiVersion: v1
          kind: Service
          name: rancher-webhook
          namespace: cattle-system

  rancher-keycloak-oidc-provider:
    info:
      description: configures Rancher for Keycloak OIDC integration
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      rancher: true
      keycloak: true
      keycloak-resources: true
      keycloak-oidc-external-secrets: true
    kustomization_spec:
      path: ./kustomize-units/rancher-keycloak-oidc-provider
      postBuild:
        substitute:
          KEYCLOAK_EXTERNAL_URL: '{{ .Values.external_hostnames.keycloak }}'
          RANCHER_EXTERNAL_URL: '{{ .Values.external_hostnames.rancher }}'
      wait: true

  k8s-gateway:
    info:
      description: installs k8s gateway (coredns + plugin to resolve external service names to ingress IPs)
      details: >
        is here only to allow for DNS resolution of Ingress hosts (FQDNs), used for importing workload clusters into Rancher and for flux-webui to use Keycloak SSO
      maturity: stable
    unit_templates:
    - base-deps
    depends_on:
      capd-metallb-config: '{{ tuple . "capd-metallb-config" | include "unit-enabled" }}'
    helm_repo_url: https://ori-edge.github.io/k8s_gateway/
    helmrelease_spec:
      chart:
        spec:
          chart: k8s-gateway
          version: 2.3.0
      targetNamespace: k8s-gateway
      install:
        createNamespace: true
      values:
        domain: '{{ .Values.cluster_domain }}'
        replicaCount: 3
        service:
          loadBalancerIP: '{{ .Values.cluster_virtual_ip }}'
          annotations:
            metallb.universe.tf/allow-shared-ip: cluster-external-ip
        # Following extraZonePlugins lines include all chart defaults plus the hosts plugin
        extraZonePlugins:
          - name: log
          - name: errors
          - name: health
            configBlock: |-
              lameduck 5s
          - name: ready
          - name: prometheus
            parameters: 0.0.0.0:9153
          - name: forward
            parameters: . /etc/resolv.conf
          - name: loop
          - name: reload
          - name: loadbalance
          - name: hosts
            configBlock: |-
              {{- $display_external_ip := .Values.display_external_ip }}
              {{- range $name,$domain := .Values.external_hostnames }}
              {{ $display_external_ip }} {{ $domain }}
              {{- end }}
              fallthrough
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: k8s-gateway
                spec:
                  template:
                    spec:
                      containers:
                      - name: k8s-gateway
                        securityContext:
                          allowPrivilegeEscalation: false
                          capabilities:
                            drop:
                            - ALL
                          runAsNonRoot: true
                          runAsGroup: 1000
                          runAsUser: 1000
                          seccompProfile:
                            type: RuntimeDefault

  capd-metallb-config:
    info:
      description: configures MetalLB in a capd context
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "metallb" | include "unit-enabled" }}'
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    repo: sylva-core
    depends_on:
      metallb: true
    kustomization_spec:
      path: ./kustomize-units/metallb-config
      wait: true
      postBuild:
        substitute:
          CLUSTER_VIRTUAL_IP: '{{ .Values.cluster_virtual_ip }}'

  capi-rancher-import:
    info:
      description: installs the capi-rancher-import operator, which let's us import Cluster AIP workload clusters in management cluster's Rancher
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    repo: capi-rancher-import
    depends_on:
      rancher: true
      k8s-gateway: true
    helmrelease_spec:
      chart:
        spec:
          chart: charts/capi-rancher-import
      targetNamespace: capi-rancher-import
      install:
        createNamespace: true
      values:
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          privileged: false
          runAsNonRoot: true
          runAsGroup: 10000
          runAsUser: 10000
          seccompProfile:
            type: RuntimeDefault
        conf:
          env:
            - name: URL_REMAP
              value: "https://{{ .Values.external_hostnames.rancher }}/ https://rancher.cattle-system.svc.cluster.local/"
            - name: REQUESTS_SSL_VERIFY
              value: "no"
          cattle_agent_kustomize_source_ref:
            kind: GitRepository
            name: capi-rancher-import
            namespace: sylva-system
          cattle_agent_kustomize_path: ./cattle-kustomize

  ingress-nginx:
    info:
      description: installs Nginx ingress controller
      maturity: stable
    unit_templates:
    - base-deps
    depends_on:
      capd-metallb-config: '{{ tuple . "capd-metallb-config" | include "unit-enabled" }}'
      calico: '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      4.5.202: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      4.6.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      releaseName: rke2-ingress-nginx
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-ingress-nginx
          version: "" # will be defined by helm_chart_versions
      values:
        fullnameOverride: rke2-ingress-nginx
        controller:
          config:
            use-forwarded-headers: true
          kind: DaemonSet
          service:
            enabled: true
            externalIPs:
            - '{{ .Values.cluster_virtual_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
          publishService:
            enabled: true
    kustomization_spec:
      healthChecks:
        - apiVersion: apps/v1
          kind: DaemonSet
          name: rke2-ingress-nginx-controller
          namespace: kube-system

  first-login-rancher:
    info:
      description: configure Rancher authentication for admin
      internal: true
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      rancher: true
    kustomization_spec:
      path: ./kustomize-units/first-login-rancher
      postBuild:
        substitute:
          RANCHER_EXTERNAL_URL: '{{ .Values.external_hostnames.rancher }}'
          CURRENT_TIME: '{{ now | date "2006-01-02T15:04:05.999Z" }}'
      wait: true

  flux-webui-init:
    info:
      description: initializes and configures flux-webui
      maturity: beta
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      sylva-ca: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.flux.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/flux-webui-init
      targetNamespace: flux-system
      postBuild:
        substitute:
          SERVICE: flux-webui
          SERVICE_DNS: '{{ .Values.external_hostnames.flux }}'
          CERT: '{{ .Values.external_certificates.flux.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.flux "cert") }}'
        - "../tls-components/sylva-ca"
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: flux-webui-tls
          namespace: flux-system

  flux-webui:
    info:
      description: installs Weave GitOps Flux web GUI
      maturity: stable
    unit_templates:
    - base-deps
    depends_on:
      flux-system: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      coredns: '{{ tuple . "keycloak" | include "unit-enabled" }}' # see https://gitlab.com/sylva-projects/sylva-core/-/merge_requests/1023#note_1694289969
      keycloak-add-client-scope: '{{ tuple . "keycloak" | include "unit-enabled" }}'
      keycloak-oidc-external-secrets: '{{ tuple . "keycloak" | include "unit-enabled" }}'
      flux-webui-init: true
    annotations:
      sylvactl/readyMessage: "Flux Web UI can be reached at https://{{ .Values.external_hostnames.flux }} ({{ .Values.external_hostnames.flux }} must resolve to {{ .Values.display_external_ip }})"
    repo: weave-gitops
    helm_chart_artifact_name: weave-gitops
    helmrelease_spec:
      chart:
        spec:
          chart: charts/gitops-server
      targetNamespace: flux-system
      install:
        createNamespace: false
      upgrade:
        force: true
      values:
        logLevel: debug
        envVars:
        - name: WEAVE_GITOPS_FEATURE_TENANCY
          value: "true"
        - name: WEAVE_GITOPS_FEATURE_CLUSTER
          value: "false"
        - name: WEAVE_GITOPS_FEATURE_OIDC_BUTTON_LABEL
          value: "Log in with Keycloak"
        installCRDs: true
        adminUser:
          create: true
          username: '{{ .Values.flux_webui.admin_user }}'
          createSecret: false
        rbac:
          impersonationResourceNames: ["admin", "sylva-admin@example.com"] # the Keycloak username set in unit keycloak-resources; cannot use "infra-admins" group here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/427
          additionalRules:
            - apiGroups: ["*"]
              resources: ["*"]
              verbs: [ "get", "list", "watch" ]
        ingress:
          enabled: true
          className: nginx
          hosts:
            - host: '{{ .Values.external_hostnames.flux }}'
              paths:
                - path: /   # setting this to another value like '/flux-webui' does not work (URLs coming back from flux webui aren't rewritten by nginx)
                  pathType: Prefix
          tls:
            - secretName: flux-webui-tls
              hosts:
                - '{{ .Values.external_hostnames.flux }}'
        extraVolumes:
          - name: custom-ca-cert
            secret:
              secretName: flux-webui-tls
              items:
                - key: ca.crt
                  path: ca.crt
        extraVolumeMounts:
          - name: custom-ca-cert
            mountPath: /etc/ssl/certs
            readOnly: true
        oidcSecret:
          create: false
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: ClusterRoleBinding
                apiVersion: rbac.authorization.k8s.io/v1
                metadata:
                  name: '{{ .Values.flux_webui.admin_user }}-user-read-resources-cr'
                subjects:
                  - apiGroup: rbac.authorization.k8s.io
                    kind: User
                    name: '{{ .Values.flux_webui.admin_user }}'
                  - apiGroup: rbac.authorization.k8s.io
                    kind: User
                    name: sylva-admin@example.com  # add same RBAC for the SSO user, so that when flux-webui SA impersonates it has privileges; cannot use "infra-admins" group here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/427
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: flux-webui-weave-gitops
                spec:
                  template:
                    spec:
                      containers:
                      - name: weave-gitops
                        securityContext:
                          allowPrivilegeEscalation: false
                          capabilities:
                            drop:
                            - ALL
                          privileged: false
                          readOnlyRootFilesystem: true
                          runAsNonRoot: true
                          runAsGroup: 1000
                          runAsUser: 1000
                          seccompProfile:
                            type: RuntimeDefault

  monitoring-crd:
    info:
      description: installs monitoring stack CRDs
      maturity: stable
      hidden: true
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
    enabled_conditions:
    - '{{ tuple . "monitoring" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring-crd
      targetNamespace: cattle-monitoring-system
      storageNamespace: cattle-monitoring-system # see https://gitlab.com/sylva-projects/sylva-core/-/issues/443
      chart:
        spec:
          chart: rancher-monitoring-crd
          version: 102.0.2+up40.1.2
      install:
        createNamespace: true

  monitoring:
    info:
      description: installs monitoring stack
      maturity: stable
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      monitoring-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring
      targetNamespace: cattle-monitoring-system
      storageNamespace: cattle-monitoring-system # see https://gitlab.com/sylva-projects/sylva-core/-/issues/443
      chart:
        spec:
          chart: rancher-monitoring
          version: 102.0.2+up40.1.2
      install:
        createNamespace: true
      values:
        grafana:
          sidecar:
            dashboards:
              enabled: true
              searchNamespace: ALL
              multicluster:
                global:
                  enabled: true
                etcd:
                  enabled: true
        prometheus:
          prometheusSpec:
            externalLabels:
              cluster: '{{ .Values.cluster.name }}'  # required for multi-cluster dashboards
            remoteWriteDashboards: true
            remoteWrite:
              - url: '{{ .Values.monitoring.thanos_receive_url }}'
                name: '{{ .Values.cluster.name }}'
                basicAuth:
                  username:
                    name: thanos-basic-auth
                    key: username
                  password:
                    name: thanos-basic-auth
                    key: password
                tlsConfig:
                  insecureSkipVerify: true
                queue_config:
                  batch_send_deadline: 5s
                  min_backoff: 1s
                  max_backoff: 30s
      # Disable drift detection for rancher-monitoring webhooks
      _postRenderers:
        - kustomize:
            patches:
            - target:
                kind: (ValidatingWebhookConfiguration|MutatingWebhookConfiguration)
              patch: |
                - op: add
                  path: /metadata/annotations/helm.toolkit.fluxcd.io~1driftDetection
                  value: disabled
    helm_secret_values:
      prometheus:
        extraSecret:
          name: thanos-basic-auth
          data:
            username: thanos-user
            password: '{{ .Values._internal.thanos_password }}'  # value from management cluster, set via Secret/shared-workload-clusters-settings

  snmp-exporter:
    info:
      description: installs SNMP exporter
      maturity: beta
    enabled_conditions:
    - '{{ tuple . "monitoring" | include "unit-enabled" }}'
    - '{{ .Values.snmp.devices | empty | not }}'
    - '{{ .Values.snmp.groups | empty | not }}'
    unit_templates:
    - base-deps
    depends_on:
      monitoring: true
    helm_repo_url: https://prometheus-community.github.io/helm-charts
    helmrelease_spec:
      releaseName: snmp-exporter
      targetNamespace: snmp-exporter
      chart:
        spec:
          chart: prometheus-snmp-exporter
          version: 1.8.1
      install:
        createNamespace: true
      values:
        fullnameOverride: 'snmp-exporter'
        replicas: 3
        extraArgs:
          - "--config.file=/config/snmp.yaml"
        serviceMonitor:
          enabled: true
          namespace: snmp-exporter
          relabelings:
            - sourceLabels: [__param_module]
              targetLabel: module
            - sourceLabels: [__param_target]
              targetLabel: instance
          params: |
            {{- $result := list -}}
            {{- range $device := .Values.snmp.devices }}
            {{- $service_monitor := dict "name" (lower $device.name) -}}
            {{- $module := list -}}
            {{- $module := append $module $device.group -}}
            {{- $_ := set $service_monitor "module" $module -}}
            {{- $_ := set $service_monitor "target" $device.ip -}}
            {{- $result = append $result $service_monitor -}}
            {{- end }}
            {{- $result | include "preserve-type" -}}
        securityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
          seccompProfile:
            type: RuntimeDefault
        containerSecurityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
        extraManifests:
          - |
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: snmp-exporter-extra-ca-certs
            data:
              extra-ca-certs.pem: | {{ .Values.oci_registry_extra_ca_certs | default "" | nindent 4 }}
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: snmp-exporter
                spec:
                  template:
                    spec:
                      containers:
                        - name: snmp-exporter
                          volumeMounts:
                            - mountPath: /config
                              name: config
                      volumes:
                        - emptyDir: {}
                          name: config
                        - name: extra-ca-certs
                          configMap:
                            name: snmp-exporter-extra-ca-certs

    helm_secret_values:
      extraInitContainers:
        - name: config-file-create
          image: registry.gitlab.com/sylva-projects/sylva-elements/container-images/sylva-toolbox:v0.3.7
          command: ["/bin/sh"]
          args:
            - -c
            - >-
              echo -e '{{ ( dict "groups" .Values.snmp.groups ) | toJson }}' > /config/groups.json &&
              sylva_snmp_resources_source='{{ .Values.sylva_core_oci_registry }}/sylva-snmp-resources' &&
              sylva_snmp_resources_version='{{ .Values.source_templates | dig "sylva-snmp-resources" "spec" "ref" "tag" true }}' &&
              flux pull artifact $sylva_snmp_resources_source:$sylva_snmp_resources_version -o /config &&
              helm template /config/sylva-snmp-resources --values /config/groups.json | yq .out > /config/snmp.yaml
          env:
            - name: http_proxy
              value: '{{ .Values.proxies.http_proxy | default "" }}'
            - name: https_proxy
              value: '{{ .Values.proxies.https_proxy | default "" }}'
            - name: no_proxy
              value: '{{ include "sylva-units.no_proxy" (tuple .) | quote }}'
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 1000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
          volumeMounts:
            - mountPath: /config
              name: config
            - name: extra-ca-certs
              subPath: extra-ca-certs.pem
              mountPath: /etc/ssl/certs/extra-ca-certs.pem
              readOnly: true

  snmp-resources:
    info:
      description: contains OID files and generates configuration needed by the snmp-exporter
      details: >
        This unit is not to be enabled, as it will not actually be consumed by sylva-units.
        It is here only to have sylva-core tools/oci/push-helm-charts-artifacts.sh
        produce an OCI artifact for it, which will be consumed directly by the `snmp-exporter`
        pods (in its init container).
      hidden: true
    enabled_conditions:
      - false
    unit_templates: []  # see above, this isn't a real unit
    repo: sylva-snmp-resources
    helm_chart_artifact_name: sylva-snmp-resources
    helmrelease_spec:
      chart:
        spec:
          chart: .

  sylva-dashboards:
    info:
      description: adds Sylva-specific Grafana dashboards
    enabled_conditions:
      - '{{ tuple . "monitoring" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      monitoring: true
    repo: sylva-dashboards
    helm_chart_artifact_name: sylva-dashboards
    helmrelease_spec:
      chart:
        spec:
          chart: .
      targetNamespace: sylva-dashboards
      install:
        createNamespace: true
      values:
        namespace: sylva-dashboards

  multus:
    info:
      description: installs Multus
      maturity: stable
    enabled: false
    unit_templates:
    - base-deps
    depends_on:
      calico: '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io/
    helmrelease_spec:
      chart:
        spec:
          chart: rke2-multus
          version: v3.9.3-build2023010901
      targetNamespace: kube-system
      install:
        createNamespace: false
      values:
        rke2-whereabouts:
          enabled: true
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: DaemonSet
                apiVersion: apps/v1
                metadata:
                  name: multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts
              - kind: ServiceAccount
                apiVersion: v1
                metadata:
                  name: multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts

  multus-ready:
    info:
      description: checks that Multus is ready
      details: >
        This unit only has dependencies, it does not create resources.
        It performs healthchecks outside of the multus unit,
        in order to properly target workload cluster when we deploy multus in it.
      internal: true
    unit_templates:
    - base-deps
    - dummy
    enabled_conditions:
      - '{{ tuple . "multus" | include "unit-enabled" }}'
    depends_on:
      multus: true
    kustomization_spec:
      healthChecks:
        - apiVersion: apps/v1
          kind: DaemonSet
          name: multus-ds
          namespace: kube-system
        - apiVersion: apps/v1
          kind: DaemonSet
          name: multus-rke2-whereabouts
          namespace: kube-system

  sriov-crd:
    info:
      description: installs SRIOV CRDs
      maturity: stable
      hidden: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "sriov" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      multus-ready: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: sriov-crd
      targetNamespace: cattle-sriov-system
      install:
        createNamespace: false
      chart:
        spec:
          chart: sriov-crd
          version: 102.1.0+up0.1.0

  sriov:
    info:
      description: installs SRIOV operator
      maturity: stable
    enabled: false
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      sriov-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: sriov
      targetNamespace: cattle-sriov-system
      chart:
        spec:
          chart: sriov
          version: 102.1.0+up0.1.0
      values:
        cert_manager: true
        rancher-nfd:
          image:
            tag: v0.13.2-build20230605

  sriov-resources:
    info:
      description: configures SRIOV resources
      internal: true
    enabled: false
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      sriov: true
    repo: sriov-resources
    helm_chart_artifact_name: sriov-resources
    helmrelease_spec:
      chart:
        spec:
          chart: .
      targetNamespace: cattle-sriov-system
      values:
        node_policies: '{{ .Values.sriov.node_policies | include "preserve-type" }}'

  coredns:
    info:
      description: configures DNS inside cluster
      internal: true
    unit_templates:
    - base-deps
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/coredns
      wait: true
      _patches:
        - target:
            kind: ConfigMap
            name: coredns
          patch: |
            - op: replace
              path: /metadata/name
              value: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpk") | ternary "coredns" "rke2-coredns-rke2-coredns" }}'
      postBuild:
        substitute:
          CLUSTER_VIRTUAL_IP: '{{ .Values.cluster_virtual_ip }}'
          CLUSTER_DOMAIN: '{{ .Values.cluster_domain }}'

  ceph-csi-cephfs:
    info:
      description: Installs Ceph-CSI
      maturity: beta
    enabled: false
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
    helm_repo_url: https://ceph.github.io/csi-charts
    helmrelease_spec:
      targetNamespace: ceph-csi-cephfs
      chart:
        spec:
          chart: ceph-csi-cephfs
          version: 3.9.0
      values:
        provisioner:
          replicaCount: '{{ .Values.cluster.control_plane_replicas }}'
        storageClass:
          create: true
          clusterID: '{{ .Values.ceph.cephfs_csi.clusterID }}'
          fsName: '{{ .Values.ceph.cephfs_csi.fs_name }}'
        csiConfig:
          - clusterID: '{{ .Values.ceph.cephfs_csi.clusterID }}'
            monitors: '{{ .Values.ceph.cephfs_csi.monitors_ips | include "preserve-type" }}'
    helm_secret_values:
      secret:
        create: true
        adminID: '{{ .Values.ceph.cephfs_csi.adminID }}'
        adminKey: '{{ .Values.ceph.cephfs_csi.adminKey }}'

  longhorn-crd:
    info:
      description: installs Longhorn CRDs
      maturity: stable
      hidden: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "longhorn" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: longhorn-crd
      targetNamespace: longhorn-system
      chart:
        spec:
          chart: longhorn-crd
          version: 103.2.1+up1.5.3

  longhorn:
    info:
      description: installs Longhorn CSI
      maturity: stable
    enabled: false
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      longhorn-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: longhorn
      targetNamespace: longhorn-system
      install:
        createNamespace: false
      chart:
        spec:
          chart: longhorn
          version: 103.2.1+up1.5.3
      values:
        defaultSettings:
          createDefaultDiskLabeledNodes: true
          allowVolumeCreationWithDegradedAvailability: false
          storageMinimalAvailablePercentage: 10

  cluster-creator-login:
    info:
      description: configures Rancher account used for workload cluster imports
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    depends_on:
      rancher: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cluster-creator-login
      wait: false
      force: true
      postBuild:
        substitute:
          JOB_NAME: cluster-creator-login
          JOB_TARGET_NAMESPACE: flux-system
          JOB_CHECKSUM: '{{ .Values | toJson | sha256sum }}'
          RANCHER_EXTERNAL_URL: '{{ .Values.external_hostnames.rancher }}'
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: cluster-creator-kubeconfig
          namespace: flux-system
        - apiVersion: batch/v1
          kind: Job
          name: cluster-creator-login-flux-system
          namespace: kube-job

  cluster-creator-policy:
    info:
      description: Kyverno policy for cluster creator
      details: |
        This units defines a Kyverno policy to distribute the Kubeconfig of cluster creator
        in all workload cluster namespaces, to allow the import of workload clusters in
        Rancher.
      internal: true
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      cluster-creator-login: true
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cluster-creator-policy
      wait: true
      force: true

  os-images-info:
    info:
      description: Creates a list of os images
      details: |
        This unit creates a configmap containing the os images (and their details in the case of Sylva diskimage-builder ones)
        to be further served by os-image-server
      internal: true
    repo: sylva-core
    enabled_conditions:
      - '{{ or (tuple . "capm3" | include "unit-enabled") (.Values.cluster.capi_providers.infra_provider | eq "capo") }}'
    unit_templates:
    - base-deps
    kustomization_spec:
      path: ./kustomize-units/kube-job
      force: true
      postBuild:
        substitute:
          JOB_NAME: create-image-info
          JOB_TARGET_NAMESPACE: '{{ .Release.Namespace }}'
          JOB_CHECKSUM: '{{ .Values._internal.os_images_info_input_hash }}'
      _patches:
        - target:
            kind: Job
          patch: |
            - op: replace
              path: /spec/template/spec/containers/0/image
              value: registry.gitlab.com/sylva-projects/sylva-elements/container-images/oci-tools:0.0.11
            - op: add
              path: /spec/template/spec/containers/0/env/-
              value:
                name: https_proxy
                value: '{{ .Values.proxies.https_proxy }}'
            - op: add
              path: /spec/template/spec/containers/0/env/-
              value:
                name: no_proxy
                value: '{{ include "sylva-units.no_proxy" (tuple .) }}'
            - op: add
              path: /spec/template/spec/containers/0/env/-
              value:
                name: oci_registry_insecure
                value: '{{ .Values.oci_registry_insecure }}'
            - op: add
              path: /spec/template/spec/containers/0/env/-
              value:
                name: OUTPUT_CONFIGMAP
                value: '{{ .Values._internal.os_images_info_configmap }}'
        - target:
            kind: ConfigMap
          patch: >-
            - op: replace
              path: /metadata/name
              value: create-image-info-{{ .Release.Namespace }}-cm
            - op: replace
              path: /data/kube-job.sh
              value: |
            {{ .Files.Get "scripts/create-os-images-info.sh" | indent 4 }}
        - target:
            kind: ConfigMap
          patch: >-
            - op: add
              path: /data/images.yaml
              value: |
            {{ include "generate-os-images" . | indent 4}}
      healthChecks:
        - apiVersion: v1
          kind: ConfigMap
          name: '{{ .Values._internal.os_images_info_configmap }}'
          namespace: '{{ .Release.Namespace }}'

  os-image-server:
    info:
      description: >
        Deploys a web server on management cluster
        which serves OS images for baremetal clusters.
      maturity: stable
    enabled_conditions:
      - '{{ tuple . .Values._internal.metal3_unit | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      '{{ .Values._internal.default_storage_class_unit }}': true
      os-images-info: true
    repo: os-image-server
    annotations:
      sylvactl/readyMessage: |
        OS images are served at:
        {{- $osImageFqdn := (coalesce .Values.external_hostnames.os_image_server .Values.display_external_ip) -}}
        {{- range $imageName, $imageParams := (coalesce .Values.os_images .Values._internal.default_os_images) }}
          * https://{{ $osImageFqdn }}/{{ $imageParams.filename }}(.sha256sum)
        {{- end }}
        {{- if not (eq $osImageFqdn .Values.display_external_ip)}}
        ({{ .Values.external_hostnames.os_image_server }} must resolve to {{ .Values.display_external_ip }})
        {{- end }}
    helmrelease_spec:
      chart:
        spec:
          chart: ./charts/os-image-server
      targetNamespace: os-images
      install:
        createNamespace: true
      timeout: 168h  # leave plenty of time to download OS images in initContainers
      values:
        downloader:
          proxy: '{{ get .Values.proxies "https_proxy" }}'
          no_proxy: '{{ include "sylva-units.no_proxy" (tuple .) }}'
          extra_ca_certs: '{{ .Values.oci_registry_extra_ca_certs | include "set-if-defined" }}'
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 10000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
        nginx:
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 10000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
        ingress:
          className: nginx
          hosts:
            - host: '{{ .Values.external_hostnames.os_image_server }}'
        osImagePersistenceDefaults:
          enabled: true
          size: '{{ .Values.os_images_default_download_storage_space }}'
          storageClass: '{{ .Values._internal.default_storage_class }}'
      valuesFrom:
        - kind: ConfigMap
          name: '{{ .Values._internal.os_images_info_configmap }}'  # this configmap is a byproduct of the os-images-info unit

  capo-contrail-bgpaas:
    info:
      description: installs CAPO Contrail BGPaaS controller
      maturity: stable
    enabled: false
    repo: capo-contrail-bgpaas
    helm_chart_artifact_name: capo-contrail-bgpaas
    unit_templates:
    - base-deps
    depends_on:
      heat-operator: '{{ tuple . "heat-operator" | include "unit-enabled" }}'
      capo: '{{ tuple . "capo" | include "unit-enabled" }}'
    helmrelease_spec:
      chart:
        spec:
          chart: .
      targetNamespace: capo-contrail-bgpaas-system
      install:
        createNamespace: true
      values:
        conf:
          env:
            DEFAULT_PORT: '0'
            DEFAULT_ASN: '64512'

  vsphere-cpi:
    info:
      description: configures Vsphere Cloud controller manager
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ eq .Values.cluster.capi_providers.infra_provider "capv" }}'
    repo: cloud-provider-vsphere
    helmrelease_spec:
      chart:
        spec:
          chart: charts/vsphere-cpi
      install:
        remediation:
          retries: -1
      releaseName: vsphere-cpi
      storageNamespace: kube-system
      targetNamespace: kube-system
      values:
        config:
          enabled: true
          vcenter: '{{ .Values.cluster.capv.server }}'
          datacenter: '{{ .Values.cluster.capv.dataCenter }}'
          tlsTumbprint: '{{ .Values.cluster.capv.tlsThumbprint }}'
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: ConfigMap
                apiVersion: v1
                metadata:
                  name: vsphere-cloud-config
                  namespace: kube-system
                data:
                  vsphere.conf: '{{ index (index .Values.vsphere "vsphere-cpi") "vsphere_conf" | toYaml }}'

    helm_secret_values:
      config:
        username: '{{ .Values.cluster.capv.username }}'
        password: '{{ .Values.cluster.capv.password }}'

  vsphere-csi-driver:
    info:
      description: installs Vsphere CSI
      maturity: stable
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ eq .Values.cluster.capi_providers.infra_provider "capv" }}'
    repo: sylva-core
    kustomization_spec:
      path: kustomize-units/vsphere-csi-driver
      targetNamespace: vmware-system-csi
      wait: true
      postBuild:
        substitute:
          SERVER: '{{ .Values.cluster.capv.server }}'
          DATACENTER: '{{ .Values.cluster.capv.dataCenter }}'
          CLUSTER_ID: '{{ printf "%s-%s" .Values.cluster.name (randAlphaNum 10) | trunc 64 }}'
          STORAGE_POLICY_NAME: '{{ .Values.cluster.capv.storagePolicyName | default "" }}'
          CONTROLLER_REPLICAS: '{{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 3 1 }}'
      _patches:
        - patch: |
            - op: replace
              path: /spec/template/spec/nodeSelector/node-role.kubernetes.io~1control-plane
              value: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "true" "" }}'
          target:
            group: apps
            version: v1
            kind: Deployment
            name: vsphere-csi-controller
            namespace: vmware-system-csi
    kustomization_substitute_secrets:
      USERNAME: '{{ .Values.cluster.capv.username }}'
      PASSWORD: '{{ .Values.cluster.capv.password }}'

  sandbox-privileged-namespace:
    info:
      description: >
        creates the sandbox namespace used
        to perform privileged operations like debugging a node
      internal: true
    unit_templates:
    - base-deps
    enabled: false
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sandbox-privileged-namespace
      wait: true
      prune: true

  # Gitea-unit
  gitea-secrets:
    info:
      description: >
        create random secret that will be used by gitea application.
        secrets are sync with vault.
      internal: true
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/gitea/secrets
      wait: true

  gitea-eso:
    info:
      description: >
        write secrets in gitea namespace in gitea expected format
      internal: true
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      eso-secret-stores: true
      gitea-keycloak-resources: true
      gitea-secrets: true
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.gitea.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/gitea/eso
      wait: false
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: gitea-keycloak-oidc-auth
          namespace: gitea
        - apiVersion: v1
          kind: Secret
          name: gitea-admin
          namespace: gitea
        - apiVersion: v1
          kind: Secret
          name: gitea-postgres-secrets
          namespace: gitea
        - apiVersion: v1
          kind: Secret
          name: gitea-redis
          namespace: gitea
        - apiVersion: v1
          kind: Secret
          name: sylva-ca.crt
          namespace: gitea
      postBuild:
        substitute:
          SERVICE: gitea
          SERVICE_DNS: '{{ .Values.external_hostnames.gitea }}'
          CERT: '{{ .Values.external_certificates.gitea.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        - '{{ ternary "../../tls-components/tls-secret" "../../tls-components/tls-certificate" (hasKey .Values.external_certificates.gitea "cert") }}'
        - "../../tls-components/sylva-ca"

  gitea-keycloak-resources:
    info:
      description: >
        deploys Gitea OIDC client in Sylva's Keycloak realm
      internal: true
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
      keycloak-resources: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/gitea/keycloak-resources
      targetNamespace: keycloak
      postBuild:
        substitute:
          GITEA_DNS: '{{ .Values.external_hostnames.gitea }}'
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-gitea-client  # this secret is a byproduct of the gitea-client KeycloakClient resource
          namespace: keycloak

  gitea-redis:
    info:
      description: installs Redis cluster for Gitea
      maturity: stable
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      gitea-eso: true
      gitea-keycloak-resources: true
      gitea-secrets: true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: redis-cluster
          version: 9.1.1
      targetNamespace: gitea
      releaseName: gitea-redis
      values:
        usePassword: true
        existingSecret: gitea-redis
        existingSecretPasswordKey: password
        global:
          storageClass: "{{ .Values._internal.default_storage_class }}"
        persistence:
          size: 8Gi

  gitea-postgresql-ha:
    info:
      description: installs PostgreSQL HA cluster for Gitea
      maturity: stable
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      gitea-eso: true
      gitea-keycloak-resources: true
      gitea-secrets: true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: postgresql-ha
          version: 11.9.8
      targetNamespace: gitea
      releaseName: gitea-postgres
      values:
        global:
          storageClass: "{{ .Values._internal.default_storage_class }}"
        postgresql:
          username: gitea
          database: gitea
          existingSecret: gitea-postgres-secrets
        pgpool:
          existingSecret: gitea-postgres-secrets
        persistence:
          size: 8Gi

  gitea:
    info:
      description: installs Gitea
      maturity: stable
    enabled: false
    unit_templates:
    - base-deps
    depends_on:
      cert-manager: true
      gitea-eso: true
      gitea-keycloak-resources: true
      gitea-secrets: true
      gitea-redis: true
      gitea-postgresql-ha: true
      namespace-defs: true
    annotations:
      sylvactl/readyMessage: "Gitea can be reached at https://{{ .Values.external_hostnames.gitea }} ({{ .Values.external_hostnames.gitea }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://dl.gitea.com/charts/
    helmrelease_spec:
      chart:
        spec:
          chart: gitea
          version: 9.5.1
      targetNamespace: gitea
      releaseName: gitea
      values:
        containerSecurityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          privileged: false
          readOnlyRootFilesystem: true
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        redis-cluster:
          enabled: false
        postgresql:
          enabled: false
        postgresql-ha:
          enabled: false
        persistence:
          enabled: true
          size: 10Gi
          storageClass: "{{ .Values._internal.default_storage_class }}"
          accessModes:
            - >-
                {{- if eq .Values._internal.default_storage_class_RWX_support "true" -}}
                  ReadWriteMany
                {{- else -}}
                  ReadWriteOnce
                {{- end -}}
        replicaCount: >-
          {{- if eq .Values._internal.default_storage_class_RWX_support "true" -}}
          {{ 3 | include "preserve-type" }}
          {{- else -}}
          {{ 1 | include "preserve-type" }}
          {{- end -}}
        strategy:
          type: >-
            {{- if eq .Values._internal.default_storage_class_RWX_support "true" -}}
              RollingUpdate
            {{- else -}}
              Recreate
            {{- end -}}
        gitea:
          admin:
            existingSecret: gitea-admin
          metrics:
            enabled: true
            serviceMonitor:
              enabled: true
          oauth:
            - name: "keycloack-sylva"
              provider: "openidConnect"
              existingSecret: gitea-keycloak-oidc-auth
              autoDiscoverUrl: 'https://{{ .Values.external_hostnames.keycloak }}/realms/sylva/.well-known/openid-configuration'

          config:
            cron:
              ENABLED: false
            cron.GIT_GC_REPOS:
              ENABLED: false
            server:
              ENABLE_PPROF: true
            database:
              DB_TYPE: postgres
              HOST: gitea-postgres-postgresql-ha-pgpool.gitea.svc.cluster.local:5432
              NAME: gitea
              USER: gitea
              # define by env variable: PASSWD
              SCHEMA: public
            session:
              PROVIDER: redis
              # define by env variable: PROVIDER_CONFIG
            cache:
              ADAPTER: redis
              # define by env variable: HOST
            queue:
              TYPE: redis
              # define by env variable: CONN_STR
            indexer:
              REPO_INDEXER_ENABLED: false
              ISSUE_INDEXER_ENABLED: false

          additionalConfigFromEnvs:
            - name: GITEA__DATABASE__PASSWD # define DB password
              valueFrom:
                secretKeyRef:
                  key: password
                  name: gitea-postgres-secrets
            - name: GITEA__QUEUE__CONN_STR # redis connection string for queue
              valueFrom:
                secretKeyRef:
                  key: connection_string
                  name: gitea-redis
            - name: GITEA__SESSION__PROVIDER_CONFIG # redis connection string for session
              valueFrom:
                secretKeyRef:
                  key: connection_string
                  name: gitea-redis
            - name: GITEA__CACHE__HOST # redis connection string for queue
              valueFrom:
                secretKeyRef:
                  key: connection_string
                  name: gitea-redis
        ingress:
          enabled: true
          className: nginx
          annotations:
            nginx.ingress.kubernetes.io/proxy-body-size: 8m
          tls:
          - hosts:
              - '{{ .Values.external_hostnames.gitea }}'
            secretName: gitea-tls
          hosts:
            - host: '{{ .Values.external_hostnames.gitea }}'
              paths:
              - path: /
                pathType: Prefix
        extraVolumes:
          - secret:
              defaultMode: 420
              secretName: sylva-ca.crt
            name: sylva-ca
        extraVolumeMounts:
          - mountPath: /etc/ssl/certs/sylva-ca.crt
            name: sylva-ca
            readOnly: true
            subPath: ca.crt

  minio-operator-init:
    info:
      description: sets up MinIO certificate for minio-operator
      details: it generate certificate
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ tuple . "minio-operator" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      sylva-ca: true
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.minio_operator.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/minio-operator-init
      wait: true
      postBuild:
        substitute:
          SERVICE: minio-operator-console
          SERVICE_DNS: '{{ .Values.external_hostnames.minio_operator_console }}'
          CERTIFICATE_NAMESPACE: minio-operator
          CERT: '{{ .Values.external_certificates.minio_operator.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.minio_operator "cert") }}'
        - "../tls-components/sylva-ca"

  minio-operator:
    info:
      description: install MinIO operator
      details: MinIO operator is used to manage multiple S3 tenants
      maturity: beta
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ tuple . "minio-monitoring-tenant" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      minio-operator-init: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
    annotations:
      sylvactl/readyMessage: "minio operator console can be reached at https://{{ .Values.external_hostnames.minio_operator_console }} ({{ .Values.external_hostnames.minio_operator_console }} must resolve to {{ .Values.display_external_ip }})"
    repo: minio-operator
    helmrelease_spec:
      targetNamespace: minio-operator
      chart:
        spec:
          chart: helm/operator
          version: v5.0.11
      install:
        createNamespace: false
      values:
        operator:
          containerSecurityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          resources:
            requests:
              cpu: 200m
              memory: 256Mi
              ephemeral-storage: 500Mi
        console:
          enabled: true
          containerSecurityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          ingress:
            enabled: true
            ingressClassName: nginx
            host: '{{ .Values.external_hostnames.minio_operator_console }}'
            path: /
            pathType: Prefix
            tls:
              - hosts:
                  - '{{ .Values.external_hostnames.minio_operator_console }}'
                secretName: minio-operator-console-tls
      _postRenderers:
        - kustomize:
            patches:
              - target:
                  kind: ClusterRole
                  name: minio-operator-role
                patch: |
                  - op: add
                    path: /rules/-
                    value:
                      apiGroups:
                        - job.min.io
                      resources:
                        - '*'
                      verbs:
                        - '*'
    kustomization_spec:
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: sts-tls
          namespace: minio-operator

  minio-monitoring-tenant-init:
    info:
      description: sets up MinIO certificate for minio-monitoring-tenant
      details: it generate certificate
      internal: true
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ tuple . "minio-monitoring-tenant" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      sylva-ca: true
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.minio_monitoring_tenant.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/minio-monitoring-tenant-init
      wait: true
      postBuild:
        substitute:
          SERVICE: minio-monitoring-tenant
          SERVICE_DNS: '{{ .Values.external_hostnames.minio_monitoring_tenant }}'
          CERTIFICATE_NAMESPACE: minio-monitoring-tenant
          CERT: '{{ .Values.external_certificates.minio_monitoring_tenant.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.minio_monitoring_tenant "cert") }}'
        - "../tls-components/sylva-ca"

  minio-monitoring-tenant:
    info:
      description: creates a MinIO tenant for the monitoring stack
      details: Loki and Thanos will use this MinIO S3 storage
      maturity: beta
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ or (tuple . "thanos" | include "unit-enabled") ( tuple . "loki" | include "unit-enabled") }}'
    depends_on:
      namespace-defs: true
      minio-operator: true
      minio-monitoring-tenant-init: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      single-replica-storageclass: '{{ tuple . "single-replica-storageclass" | include "unit-enabled" }}'
    annotations:
      sylvactl/readyMessage: MinIO monitoring tenant console can be reached at https://{{ .Values.external_hostnames.minio_monitoring_tenant_console }} ({{ .Values.external_hostnames.minio_monitoring_tenant_console }} must resolve to {{ .Values.display_external_ip }})
    repo: minio-operator
    helmrelease_spec:
      targetNamespace: minio-monitoring-tenant
      chart:
        spec:
          chart: helm/tenant
          version: v5.0.11
      install:
        createNamespace: false
      values:
        secrets:
          # we need existingSecret to be empty in order to have unit create Secret/minio-root-account out of .accessKey & .secretKey
          existingSecret: {}
          name: minio-root-account
        tenant:
          name: monitoring
          configuration:
            name: minio-root-account
          pools:
            - servers: 2
              name: pool-0
              volumesPerserver: 4
              size: 3Gi
              storageClassName: '{{ tuple . (tuple . "single-replica-storageclass" | include "unit-enabled") "single-replica-storageclass" .Values._internal.default_storage_class | include "interpret-ternary" }}'
              containerSecurityContext:
                runAsUser: 1000
                runAsGroup: 1000
                runAsNonRoot: true
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
                seccompProfile:
                  type: RuntimeDefault
              securityContext:
                runAsUser: 1000
                runAsGroup: 1000
                fsGroup: 1000
                fsGroupChangePolicy: "OnRootMismatch"
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
          metrics:
            enabled: disable
          features:
            bucketDNS: false
            enableSFTP: false
          users:
            # Secrets having as contents
            # - CONSOLE_ACCESS_KEY - The "Username" for the MinIO user
            # - CONSOLE_SECRET_KEY - The "Password" for the MinIO user
            - name: storage-user
          buckets:
            - name: "thanos"
              region: "monitoring-cluster"
              objectLock: false
            - name: "loki"
              region: "monitoring-cluster"
              objectLock: false
            - name: "loki-chunks"
              region: "monitoring-cluster"
              objectLock: false
            - name: "loki-ruler"
              region: "monitoring-cluster"
              objectLock: false
            - name: "loki-admin"
              region: "monitoring-cluster"
              objectLock: false
            - name: "testbucket"
              region: "monitoring-cluster"
              objectLock: false
          prometheusOperator: false    # Prometheus Operator's Service Monitor for MinIO Tenant Pods
          logging:
            anonymous: true
            json: true
            quiet: true
        ingress:
          api:
            enabled: true
            ingressClassName: nginx
            annotations:
              nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
            host: '{{ .Values.external_hostnames.minio_monitoring_tenant }}'
            path: /
            pathType: Prefix
            tls:
              - hosts:
                  - '{{ .Values.external_hostnames.minio_monitoring_tenant }}'
                secretName: minio-monitoring-tenant-tls
          console:
            enabled: true
            ingressClassName: nginx
            annotations:
              nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
            host: '{{ .Values.external_hostnames.minio_monitoring_tenant_console }}'
            path: /
            pathType: Prefix
            tls:
              - hosts:
                  - '{{ .Values.external_hostnames.minio_monitoring_tenant_console }}'
                secretName: minio-monitoring-tenant-tls
    helm_secret_values:
      secrets:
        accessKey: minio
        secretKey: '{{ .Values._internal.minio_monitoring_root_password }}'
      extraResources:
        - |
          apiVersion: v1
          kind: Secret
          type: Opaque
          metadata:
            name: storage-user
          stringData:
            CONSOLE_ACCESS_KEY: console
            CONSOLE_SECRET_KEY: '{{ .Values._internal.minio_monitoring_user_password }}'
    kustomization_spec:
      # these healthChecks are added so that MinIO backend setup
      # for tenant.minio.min.io/monitoring created by this unit is ready
      healthChecks:
        - apiVersion: apps/v1
          kind: StatefulSet
          name: monitoring-pool-0  # concatenation from .tenant chart values .name & .pools[*].name
          namespace: minio-monitoring-tenant
        - apiVersion: v1
          kind: Secret
          name: monitoring-tls
          namespace: minio-monitoring-tenant

  thanos-init:
    info:
      description: sets up thanos certificate
      details: it generates a multiple CN certificate for all Thanos components
      internal: true
    enabled_conditions:
      - '{{ tuple . "thanos" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      sylva-ca: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/thanos-init
      wait: true
      postBuild:
        substitute:
          SERVICE: thanos
          THANOS_DNS: '{{ .Values.external_hostnames.thanos }}'
          THANOS_RECEIVE_DNS: '{{ .Values.external_hostnames.thanos_receive }}'
          THANOS_STOREGATEWAY_DNS: '{{ .Values.external_hostnames.thanos_storegateway }}'
          THANOS_QUERY_DNS: '{{ .Values.external_hostnames.thanos_query }}'

  thanos:
    info:
      description: installs Thanos
      maturity: beta
    enabled_conditions:
      - '{{ tuple . "monitoring" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      thanos-init: true
      minio-monitoring-tenant: true
      '{{ .Values._internal.default_storage_class_unit }}': true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
    annotations:
      sylvactl/readyMessage: Thanos UI can be reached at https://{{ .Values.external_hostnames.thanos }} ({{ .Values.external_hostnames.thanos }} must resolve to {{ .Values.display_external_ip }})
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      targetNamespace: thanos
      install:
        createNamespace: false
      chart:
        spec:
          chart: thanos
          version: 12.16.1
      values:
        fullnameOverride: "thanos"
        metrics:
          enabled: true
          serviceMonitor:
            enabled: true
        query:
          enabled: true
          logLevel: debug
          dnsDiscovery:
            enabled: true
          extraFlags:
            - --query.auto-downsampling
            - --query.replica-label=prometheus_replica
            - --query.replica-label=prometheus
          ingress:
            enabled: true
            ingressClassName: "nginx"
            hostname: '{{ .Values.external_hostnames.thanos_query }}'
            extraTls:
              - hosts:
                  - 'thanos-query.{{ .Values.external_hostnames.thanos_query }}'
                secretName: thanos-tls
          containerSecurityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
        queryFrontend:
          enabled: true
          loglevel: debug
          config: |-
            type: IN-MEMORY
            config:
              max_size: 1GB
              max_size_items: 0
              validity: 0s
          ingress:
            enabled: true
            ingressClassName: "nginx"
            hostname: '{{ .Values.external_hostnames.thanos }}'
            extraTls:
              - hosts:
                  - '{{ .Values.external_hostnames.thanos }}'
                secretName: thanos-tls
          containerSecurityContext:
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
        storegateway:
          enabled: true
          logLevel: debug
          persistence:
            enabled: false
            storageClass: '{{ .Values._internal.default_storage_class }}'
            size: 8Gi
          containerSecurityContext:
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          podSecurityContext:
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          ingress:
            enabled: true
            hostname: '{{ .Values.external_hostnames.thanos_storegateway }}'
            ingressClassName: "nginx"
            extraTls:
              - hosts:
                  - '{{ .Values.external_hostnames.thanos_storegateway }}'
                secretName: thanos-tls
        compactor:
          enabled: true
          logLevel: debug
          retentionResolutionRaw: 2d
          retentionResolution5m: 30d
          retentionResolution1h: 1y
          containerSecurityContext:
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          podSecurityContext:
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
        bucketweb:
          enabled: false
        ruler:
          enabled: false
          logLevel: debug
          dnsDiscovery:
            enabled: true
          containerSecurityContext:
            runAsUser: 1001
            runAsNonRoot: true
            privileged: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          podSecurityContext:
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
        receive:
          enabled: true
          mode: standalone
          logLevel: debug
          persistence:
            enabled: false
          containerSecurityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            seccompProfile:
              type: RuntimeDefault
          ingress:
            enabled: true
            ingressClassName: "nginx"
            hostname: '{{ .Values.external_hostnames.thanos_receive }}'
            extraTls:
              - hosts:
                  - '{{ .Values.external_hostnames.thanos_receive }}'
                secretName: thanos-tls
    helm_secret_values:
      auth:
        basicAuthUsers:
          thanos-user: '{{ .Values._internal.thanos_password }}'
      objstoreConfig: |-
        type: "S3"
        config:
          bucket: "thanos"
          endpoint: minio.minio-monitoring-tenant.svc.cluster.local
          access_key: "minio"
          secret_key: '{{ .Values._internal.minio_monitoring_root_password }}'
          insecure: false
          http_config:
            tls_config:
              insecure_skip_verify: true
            insecure_skip_verify: true

  logging-crd:
    info:
      description: install rancher-logging CRD
      maturity: beta
      hidden: true
    enabled_conditions:
      - '{{ tuple . "logging" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: logging-crd
      targetNamespace: cattle-logging-system
      chart:
        spec:
          chart: rancher-logging-crd
          version: 102.0.1+up3.17.10
      install:
        createNamespace: false

  logging:
    enabled: false
    info:
      description: installs Rancher Fluentbit/Fluentd logging stack, for log collecting and shipping
      maturity: beta
    unit_templates:
    - base-deps
    depends_on:
      logging-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: logging
      targetNamespace: cattle-logging-system
      chart:
        spec:
          chart: rancher-logging
          version: 102.0.1+up3.17.10
      install:
        createNamespace: false
      values:
        additionalLoggingSources:
          rke2:
            enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | include "as-bool" }}'
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop: ["ALL"]
          seccompProfile:
            type: RuntimeDefault

  logging-config:
    info:
      description: Configures rancher-logging to ship logs to Loki
      internal: true
    enabled_conditions:
    - '{{ tuple . "logging" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      logging: true
      #loki: this dependency is handled in a different way, in management.values.yaml and workload-cluster.values.yaml
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/logging
      postBuild:
        substitute:
          # we must use external name in order to work also for workload clusters
          LOKI_URL: '{{ .Values.logging.loki_url }}'
          CLUSTER_NAME: '{{ .Values.cluster.name }}'
          LOKI_USERNAME: "loki-user"
          LOKI_PASSWORD: '{{ .Values._internal.loki_password }}'
      wait: true

  loki-init:
    info:
      description: sets up Loki certificate
      details: it generate certificate
      internal: true
    enabled_conditions:
      - '{{ tuple . "loki" | include "unit-enabled" }}'
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      sylva-ca: true
    repo: sylva-core
    kustomization_substitute_secrets:
      KEY: '{{ .Values.external_certificates.loki.key | default "" | b64enc }}'
    kustomization_spec:
      path: ./kustomize-units/loki-init
      wait: true
      postBuild:
        substitute:
          SERVICE: loki
          SERVICE_DNS: '{{ .Values.external_hostnames.loki }}'
          CERTIFICATE_NAMESPACE: loki
          CERT: '{{ .Values.external_certificates.loki.cert | default "" | b64enc }}'
          CACERT: '{{ .Values.external_certificates.cacert | default "" | b64enc }}'
      _components:
        - '{{ ternary "../tls-components/tls-secret" "../tls-components/tls-certificate" (hasKey .Values.external_certificates.loki "cert") }}'
        - "../tls-components/sylva-ca"

  loki:
    enabled: false
    info:
      description: installs Loki log storage
      details: installs Loki log storage in simple scalable mode
      maturity: beta
    unit_templates:
    - base-deps
    depends_on:
      namespace-defs: true
      loki-init: true
      minio-monitoring-tenant: true
    enabled_conditions:
      - '{{ tuple . "logging" | include "unit-enabled" }}'
    annotations:
      sylvactl/readyMessage: "Loki can be reached at https://{{ .Values.external_hostnames.loki }} ({{ .Values.external_hostnames.loki }} must resolve to {{ .Values.display_external_ip }})"
    repo: loki
    helmrelease_spec:
      targetNamespace: loki
      chart:
        spec:
          chart: production/helm/loki
          version: v2.9.2
      install:
        createNamespace: false
      values:
        global:
          dnsService: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "rke2-coredns-rke2-coredns" "kube-dns" }}'
        sidecar:
          rules:
            enabled: false
        loki:
          auth_enabled: true
          containerSecurityContext:
            seccompProfile:
              type: RuntimeDefault
          analytics:
            reporting_enabled: false
          server:
            http_listen_port: 3100
            grpc_listen_port: 9095
            http_server_idle_timeout: 600s
            http_server_read_timeout: 600s
            http_server_write_timeout: 600s
            grpc_server_max_recv_msg_size: 52428800
            grpc_server_max_send_msg_size: 52428800
          # -- Limits config
          limits_config:
            enforce_metric_name: false
            reject_old_samples: false
            reject_old_samples_max_age: 168h
            max_cache_freshness_per_query: 10m
            split_queries_by_interval: 15m
            query_timeout: 10m
            ingestion_rate_strategy: local
            ingestion_rate_mb: 15
            ingestion_burst_size_mb: 20
            unordered_writes: true
          # todo
          ingester:
            chunk_block_size: 262144
            chunk_idle_period: 3m
            chunk_retain_period: 1m
            max_transfer_retries: 0
            wal:
              enabled: true
              dir: /data/wal
          commonConfig:
            replication_factor: 1
          storage:
            bucketNames:
              chunks: "loki-chunks"
              ruler: "loki-ruler"
              admin: "loki-admin"
            type: s3
            s3:
              s3: null
              endpoint: "minio.minio-monitoring-tenant.svc.cluster.local:443"
              region: null
              # accessKeyId and secretAccessKey are passed via helm_secret_values, see below
              signatureVersion: null
              s3ForcePathStyle: true
              insecure: false
              http_config:
                idle_conn_timeout: 90s
                insecure_skip_verify: true
        test:
          enabled: false
        monitoring:
          dashboards:
            enabled: true
            labels:
              grafana_dashboard: "1"
          rules:
            enabled: true
            alerting: true
            additionalGroups:
              - name: additional-loki-rules
                rules:
                  - record: job:loki_request_duration_seconds_bucket:sum_rate
                    expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)
                  - record: job_route:loki_request_duration_seconds_bucket:sum_rate
                    expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)
                  - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
                    expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)
            podsecurityContext:
              seccompProfile:
                type: RuntimeDefault
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - ALL
              runAsNonRoot: true
              seccompProfile:
                type: RuntimeDefault
          serviceMonitor:
            enabled: true
            metricsInstance:
              enabled: true
          # todo: need GrafanaAgent
          selfMonitoring:
            enabled: false
            tenant:
              name: "self-monitoring"
            grafanaAgent:
              installOperator: false
          lokiCanary:
            enabled: false
            labelname: pod
            securityContext:
              allowPrivilegeEscalation: false
              capabilities:
                drop:
                  - ALL
              runAsNonRoot: true
              seccompProfile:
                type: RuntimeDefault
        write:
          replicas: '{{ int .Values._internal.node_count | gt 2 | ternary 1 2 }}'
          persistence:
            size: 3Gi
          autoscaling:
            enable: true
            minReplicas: 2
            maxReplicas: 6
            targetCPUUtilizationPercentage: 40
          podSecurityContext:
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
          extraVolumes:
            - name: data
              emptyDir: {}
            - name: loki
              emptyDir: {}
          extraVolumeMounts:
            - name: data
              mountPath: /data
            - name: loki
              mountPath: /loki
        tablemanager:
          enabled: false
        read:
          replicas: '{{ int .Values._internal.node_count | gt 2 | ternary 1 2 }}'
          persistence:
            size: 3Gi
          podSecurityContext:
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
        backend:
          replicas: '{{ int .Values._internal.node_count | gt 2 | ternary 1 2 }}'
          persistence:
            size: 3Gi
          podSecurityContext:
            seccompProfile:
              type: RuntimeDefault
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            runAsNonRoot: true
            seccompProfile:
              type: RuntimeDefault
        memberlist:
          service:
            publishNotReadyAddresses: true
          join_members:
            - loki-memberlist
        gateway:
          enabled: true
          verboseLogging: true
          replicas: '{{ int .Values._internal.node_count | gt 2 | ternary 1 2 }}'
          ingress:
            enabled: true
            ingressClassName: "nginx"
            annotations:
              nginx.ingress.kubernetes.io/proxy-body-size: "0"
            hosts:
              - host: '{{ .Values.external_hostnames.loki }}'
                paths:
                  - path: /
                    pathType: Prefix
            tls:
              - secretName: loki-tls
                hosts:
                  - '{{ .Values.external_hostnames.loki }}'
          basicAuth:
            enabled: true
            # (username and password set under helm_secret_values)
          containerSecurityContext:
            seccompProfile:
              type: RuntimeDefault
    helm_secret_values:
      gateway:
        basicAuth:
          username: loki-user
          password: '{{ .Values._internal.loki_password }}'
      loki:
        storage:
          s3:
            accessKeyId: "console"
            secretAccessKey: '{{ .Values._internal.minio_monitoring_user_password }}'

  single-replica-storageclass:
    info:
      description: Create a longhorn storage class with a single replica
      internal: true
    repo: sylva-core
    unit_templates:
    - base-deps
    depends_on:
      longhorn: true
    enabled_conditions:
      - '{{ tuple . "minio-monitoring-tenant" | include "unit-enabled" }}'
      - '{{ tuple . "longhorn" | include "unit-enabled" }}'
    kustomization_spec:
      path: ./kustomize-units/longhorn-storageclass
      wait: true
      postBuild:
        substitute:
          CLASS_NAME: single-replica-storageclass
      _patches:
        - target:
            kind: StorageClass
          patch: |
            kind: _unused_
            metadata:
              name: _unused_
            parameters:
              numberOfReplicas: "1"
              dataLocality: "best-effort"

  sylva-prometheus-rules:
    info:
      description: installs prometheus rules using external helm chart & rules git repo
      maturity: beta
    unit_templates:
    - base-deps
    enabled_conditions:
      - '{{ tuple . "monitoring" | include "unit-enabled" }}'
    repo: sylva-prometheus-rules
    helmrelease_spec:
      releaseName: sylva-prometheus-rules
      targetNamespace: sylva-prometheus-rules
      chart:
        spec:
          chart: prometheus-rules
      install:
        createNamespace: true
      values:
        createRules:
          allclusters: true
          '{{ .Values.cluster.name }}': true

  get-openstack-images:
    info:
      description: Automatically push openstack images to cinder
      internal: true
    unit_templates:
    - base-deps
    depends_on:
      os-images-info: true
    enabled_conditions:
      - '{{ eq .Values.cluster.capi_providers.infra_provider "capo" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/get-openstack-images
      wait: true
      force: true
      postBuild:
        substitute:
          JOB_NAME: get-openstack-images
          JOB_TARGET_NAMESPACE: '{{ .Release.Namespace }}'
          JOB_ORAS_INSECURE_CLIENT: '{{ .Values.oci_registry_insecure | quote }}'
          JOB_CHECKSUM:
            '{{
            list
              .Values._internal.sylva_core_version
              (mergeOverwrite (deepCopy .Values.os_images) .Values.sylva_diskimagebuilder_images)
              .Values.sylva_diskimagebuilder_version
              .Values.sylva_base_oci_registry
              .Values.proxies
            | toJson | sha256sum
            }}'
          httpProxy: '{{ .Values.proxies.https_proxy }}'
          httpsProxy: '{{ .Values.proxies.https_proxy }}'
          noProxy: '{{ include "sylva-units.no_proxy" (tuple .) }}'
          TMP_STORAGE_SPACE: '{{ .Values.os_images_default_download_storage_space }}'
          # compute total timeout based on per-image timeout and number of images
          ACTIVE_DEADLINE_SECONDS: '{{ mul .Values.get_openstack_images_per_image_timeout_minutes 60 (include "generate-os-images" . | fromYaml | dig "osImages" dict | len) }}'
      _patches:
        - target:
            kind: Job
          patch: |
            - op: replace
              path: /spec/template/spec/volumes/2/configMap/name
              value: {{ .Values._internal.os_images_info_configmap }}  # this configmap is a byproduct of the os-images-info unit

##### stuff related to the 'cluster' unit #####
#
# all these values under 'cluster' are passed as values to sylva-capi-cluster chart

cluster:
  name: management-cluster

  # can be set to true to do an RKE2 deployment disconnected from the Internet:
  air_gapped: false

  # cis profile to be used. Curently supported only for rke2 clusters. "cis-1.6" for k8s prior to 1.25, "cis-1.23" for 1.25+
  cis_profile: cis-1.23

  # for now, the choice below needs to be made
  # consistently with the choice of a matching kustomization path
  # for the 'cluster' unit
  # e.g. you can use ./management-cluster-def/rke2-capd
  capi_providers:
    infra_provider: capd      # capd, capo, capm3 or capv
    bootstrap_provider: cabpk # cabpr (RKE2) or cabpk (kubeadm)

  # kubernetes version to be used
  k8s_version: '{{ .Values._internal.k8s_version_map | dig .Values.k8s_version_short "" | required (printf "no k8s version defined for %s" .Values.k8s_version_short) }}'

  # kube_vip version to be used for kubeadm deployments
  images:
    kube_vip:
      repository: ghcr.io/kube-vip/kube-vip
      tag: "v0.7.0"

  # Nodes number for control-plane
  control_plane_replicas: 3

  kubelet_extra_args:
    # increase max-pods for single-node deployments
    max-pods: '{{ tuple "196" (eq (.Values._internal.node_count | int) 1) | include "set-only-if" }}'

  capo:
    # flavor_name: m1.large # Openstack flavor name
    # image_key: # key of an image in os_images or sylva_diskimage_builder
    # image_name: # (deprecated, please use image_key instead) - OpenStack image name (one of image_key and image_name must be set, but not both)
    # ssh_key_name: # OpenStack VM SSH key
    # network_id: # OpenStack network used for nodes and VIP port
    # rootVolume: {} # Let this parameter empty if you don't intent to use root volume
    #   # otherwise, provide following values
    #   diskSize: 20 # Size of the VMs root disk
    #   volumeType: '__DEFAULT__' # Type of volume to be created
    # #control_plane_az: # list of OpenStack availability zones to deploy control planes nodes to, otherwise all would be candidates
    # clouds_yaml: # (this is a dict, not a YAML string)
    #   clouds:
    #     capo_cloud:
    #       auth:
    #         auth_url: # replace me
    #         user_domain_name: # replace me
    #         project_domain_name: # replace me
    #         project_name: # replace me
    #         username: # replace me
    #         password: # replace me
    #       region_name: # replace me
    #       verify: # e.g. false
    # cacert: # cert used to validate CA of OpenStack APIs

    # tag set for OpenStack resources in management cluster:
    resources_tag: >-
      {{- if .Values.cluster.capi_providers.infra_provider | eq "capo" -}}
        sylva-{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username }}
      {{- end -}}

  control_plane:
    capo:
      security_group_names:
      - capo-{{ .Values.cluster.name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}
      - capo-{{ .Values.cluster.name }}-security-group-common-{{ .Values.cluster.capo.resources_tag }}

  machine_deployment_default:
    machine_deployment_spec:
      strategy:
        rollingUpdate:
          maxUnavailable: 1
          # use maxSurge 0 for baremetal deployments
          maxSurge: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" | ternary 0 1 | include "preserve-type" }}'
    capo:
      security_group_names:
      - capo-{{ .Values.cluster.name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}
      - capo-{{ .Values.cluster.name }}-security-group-common-{{ .Values.cluster.capo.resources_tag }}

  cluster_virtual_ip: '{{ .Values.cluster_virtual_ip }}'

  cluster_public_ip: '{{ tuple (.Values.openstack.floating_ip) (not (eq .Values.openstack.floating_ip "")) | include "set-only-if" }}'
  cluster_services_cidrs:
    - 100.73.0.0/16
  cluster_pods_cidrs:
    - 100.72.0.0/16
  calico_wireguard_enabled: '{{ .Values.calico_wireguard_enabled | include "preserve-type" }}'

  use_custom_rancher_dns_resolver: false
  ntp: '{{ .Values.ntp | include "preserve-type" }}'
  proxies:
    http_proxy: '{{ .Values.proxies.http_proxy }}'
    https_proxy: '{{ .Values.proxies.https_proxy }}'
    no_proxy: '{{ include "sylva-units.no_proxy" (tuple .) }}'

  registry_mirrors: '{{ .Values.registry_mirrors | include "preserve-type" }}'

  metallb_helm_oci_url: ""  # for an OCI-based deployment this is overridden in use-oci-artifacts.values.yaml
  metallb_helm_version: '{{ .Values.units.metallb.helmrelease_spec.chart.spec.version }}'
  metallb_helm_extra_ca_certs: ""  # for an OCI-based deployment this is overridden in use-oci-artifacts.values.yaml

  capv:
    # image_name: # vSphere image name
    # username: ""
    # password: ""
    # dataCenter: ""
    # server: ""
    # dataStore: ""
    # tlsThumbprint: ""
    # folder: ""
    # resourcePool: ""
    # storagePolicyName: ""
    # networks:
    #   default:
    #     networkName: ""
    # ssh_key: ''
    numCPUs: 4

  capm3:
    machine_image_checksum_type: sha256
    image_provisioning_host: '{{ .Values.display_external_ip }}'

  enable_longhorn: '{{ or (tuple . "longhorn" | include "unit-enabled") (tuple . .Values.units.longhorn.enabled | include "interpret-for-test") | include "as-bool" }}'

  default_node_class: generic
  node_classes: {}
    # generic:
    #   kernel_cmdline:
    #     hugepages:
    #       enabled: false
    #       2M_percentage_total: ""
    #       1G_percentage_total: ""
    #       default_size: 2M
    #     extra_options: ""
    #   kubelet_extra_args: {}
    #   kubelet_config_file_options: {}
    #   nodeTaints: {}
    #   nodeLabels: {}
    #   nodeAnnotations: {}
    #   additional_commands:
    #     pre_bootstrap_commands: []
    #     post_bootstrap_commands: []

capd_docker_host: unix:///var/run/docker.sock

openstack:
  #external_network_id # Can be provided if a FIP is needed in order to reach the management cluster VIP
  floating_ip: ""  # will typically be set by capo-cluster-resources
  storageClass:
    name: cinder-csi  # name of the storageClass to be created
    #type: xxx  # please provide the cinder volume type, e.g. 'ceph_sas' (must exist in OpenStack)
  control_plane_affinity_policy: soft-anti-affinity
  worker_affinity_policy: soft-anti-affinity

oci_registry_insecure: false

metal3:
  provider: suse # sylva or suse
  # external_bootstrap_ip:
  # bootstrap_ip:

vsphere:
  vsphere-cpi:
    vsphere_conf:
      # Global properties in this section will be used for all specified vCenters unless overriden in VirtualCenter section.
      global:
        port: 443
        # set insecure-flag to true if the vCenter uses a self-signed cert
        insecureFlag: true
        # settings for using k8s secret
        secretName: vsphere-cloud-secret
        secretNamespace: kube-system

      # vcenter section
      vcenter:
        '{{ .Values.cluster.capv.server }}':
          server: '{{ .Values.cluster.capv.server }}'
          user: '{{ .Values.cluster.capv.username }}'
          password: '{{ .Values.cluster.capv.password }}'
          datacenters:
            - '{{ .Values.cluster.capv.dataCenter }}'

cluster_virtual_ip: 55.55.55.55

# Admin password that will be configured by default on various units  # FIXME, only used for SSO today see https://gitlab.com/sylva-projects/sylva-core/-/issues/503
# [WARNING] This value cannot be overwritten on production environment (env_type: prod)
admin_password: '{{ .Values._internal.default_password }}'

flux_webui:
  admin_user: admin

display_external_ip: '{{ .Values.openstack.floating_ip | eq "" | ternary .Values.cluster_virtual_ip .Values.openstack.floating_ip }}'

cluster_domain: sylva

external_hostnames:
  rancher: 'rancher.{{ .Values.cluster_domain }}'
  vault: 'vault.{{ .Values.cluster_domain }}'
  keycloak: 'keycloak.{{ .Values.cluster_domain }}'
  flux: 'flux.{{ .Values.cluster_domain }}'
  neuvector: 'neuvector.{{ .Values.cluster_domain }}'
  harbor: 'harbor.{{ .Values.cluster_domain }}'
  os_image_server: ''
  gitea: 'gitea.{{ .Values.cluster_domain }}'
  minio_operator_console: 'minio-operator-console.{{ .Values.cluster_domain }}'
  minio_monitoring_tenant: 'minio-monitoring-tenant.{{ .Values.cluster_domain }}'
  minio_monitoring_tenant_console: 'minio-monitoring-tenant-console.{{ .Values.cluster_domain }}'
  thanos: 'thanos.{{ .Values.cluster_domain }}'
  thanos_storegateway: 'thanos-storegateway.{{ .Values.cluster_domain }}'
  thanos_receive: 'thanos-receive.{{ .Values.cluster_domain }}'
  thanos_query: 'thanos-query.{{ .Values.cluster_domain }}'
  loki: 'loki.{{ .Values.cluster_domain }}'

external_certificates:
  rancher: {}
  vault: {}
  keycloak: {}
  flux: {}
  neuvector: {}
  harbor: {}
  os_image_server: {}
  gitea: {}
  minio_operator: {}
  minio_monitoring_tenant: {}
  thanos: {}
  loki: {}

audit_log:
  level: 0

keycloak: {}

# cis benchmark is only for rke2 so far, e.g. rke2-cis-1.23-profile-hardened
cis_benchmark_scan_profile: '{{ eq .Values.cluster.capi_providers.bootstrap_provider "cabpr" | ternary "rke2-cis-1.23-profile-hardened" "no-scan-profile-defined-for-kubeadm-cluster" }}'

# osImages images that should be served and from where they should be downloaded
# if empty default value are used
os_images: {}

# to configure the SR-IOV VFs on the supported NICs of cluster nodes
sriov:
  node_policies: {}
#   mypolicy1:
#     nodeSelector: {}  # <<< lets you further limit the SR-IOV capable nodes on which the VFs have to be created in a certain config; if not set it applies to all SR-IOV nodes
#     resourceName: ""
#     numVfs: ""
#     deviceType: ""  # supported values: "netdevice" or "vfio-pci"
#     nicSelector:
#       deviceID: ""
#       vendor: ""
#       pfNames: []
#       rootDevices: []

# add ceph cluster details
ceph:
  cephfs_csi:
    clusterID: ""
    fs_name: ""
    adminID: ""
    adminKey: ""
    monitors_ips: []

# add your proxy settings if required
proxies:
  https_proxy: ""
  http_proxy: ""
  no_proxy: ""  # you can also use no_proxy_additional, see below

# you can disable default values for no_proxy (localhost,.svc,.cluster.local.,.cluster.local,.sylva)
# Ex: localhost: false
no_proxy_additional:
  10.0.0.0/8: true
  192.168.0.0/16: true
  172.16.0.0/12: true
  localhost: true
  127.0.0.1: true
  .svc: true
  '{{ printf ".%s" .Values.cluster_domain }}': true
  .cluster.local.: true
  .cluster.local: true

# configure containerd registry mirrors following https://github.com/containerd/containerd/blob/main/docs/hosts.md
registry_mirrors:
  default_settings:                          # <<< These settings will apply to all configured mirrors
    capabilities: ["pull", "resolve"]
#   skip_verify: true
#   override_path: true
# hosts_config:
#   docker.io:
#   - mirror_url: http://your.mirror/docker
#     registry_settings:                     # <<< Host settings can be used to override default_settings
#       skip_verify: false
#   registry.k8s.io:
#   - mirror_url: ...
#   _default:
#   - mirror_url: ...

# deploy emulated baremetal nodes in bootstrap cluster
libvirt_metal:
  image: registry.gitlab.com/sylva-projects/sylva-elements/container-images/libvirt-metal:{{ .Values.source_templates | dig "libvirt-metal" "spec" "ref" "tag" "_undefined_"}}
  nodes: {}
  #management-cp:
  #  memGB: 12
  #  numCPUs: 6
  #
  #workload-cp:
  #  memGB: 4
  #  numCPUs: 2
  #
  #workload-md:
  #  memGB: 2
  #  numCPUs: 2


# set the type of environment between 3 possible values: dev, ci and prod
env_type: prod

# set NTP servers by IP or FQDN and enable their usage for control plane nodes
ntp:
  enabled: false
  servers:
    - 1.2.3.4
    - europe.pool.ntp.org

# These two sylva_core_oci_registry/sylva_base_oci_registry values govern which OCI repos are used
#
# This matters for:
# 1) OCI deployments
# 2) for non-OCI deployments for retrieving artifacts such as metal3 OS images
#
# For (1) sylva_base_oci_registry is automatically derived from the OCI repo used for sylva-unit HelmRelease.
# For (2) sylva_base_oci_registry can be customized, to use an OCI registry other than registry.gitlab.com/sylva-projects
#
# It should in general not be any need to override sylva_core_oci_registry, which is derived
# from sylva_base_oci_registry.
#
# sylva_base_oci_registry defaults to oci://registry.gitlab.com/sylva-projects
sylva_base_oci_registry:
  '{{
  regexReplaceAll
    "/sylva-core/?$"
    (lookup "source.toolkit.fluxcd.io/v1beta2" "HelmRepository" .Release.Namespace "sylva-core" | dig "spec" "url" "")
    ""
  | default "oci://registry.gitlab.com/sylva-projects"
  }}'
sylva_core_oci_registry: '{{ .Values.sylva_base_oci_registry }}/sylva-core'

k8s_version_short: "1.27"

# Renovate Bot needs additional information to detect sylva diskimage-builder version
# renovate: depName=sylva-projects/sylva-elements/diskimage-builder
sylva_diskimagebuilder_version: 0.1.8

sylva_diskimagebuilder_images:
  ubuntu-jammy-plain-rke2-1-27-6:
    default_enabled: true
  ubuntu-jammy-plain-rke2-1-26-9: {}
  ubuntu-jammy-plain-rke2-1-25-14: {}
  ubuntu-jammy-hardened-rke2-1-27-6: {}
  ubuntu-jammy-hardened-rke2-1-26-9: {}
  ubuntu-jammy-hardened-rke2-1-25-14: {}
  opensuse-15-5-plain-rke2-1-27-6: {}
  opensuse-15-5-plain-rke2-1-26-9: {}
  opensuse-15-5-plain-rke2-1-25-14: {}
  opensuse-15-5-hardened-rke2-1-27-6: {}
  opensuse-15-5-hardened-rke2-1-26-9: {}
  opensuse-15-5-hardened-rke2-1-25-14: {}

# this dict can be enriched at deployment time to feed additional information
# into the sylva-units-status ConfigMap
additional_sylva_units_status_info: {}

# os_images_default_download_storage_space defines the default size of volumes
# used by os-image-server and get-openstack-images to download each OS image OCI artifacts
# and uncompress the image stored inside
#
# it needs to cover the size of the OCI artifact *plus* the size of the uncompressed image
#
# this needs to be set to ~25Gi for hardened images
#    (until https://gitlab.com/sylva-projects/sylva-elements/diskimage-builder/-/issues/57 is addressed)
os_images_default_download_storage_space: 8Gi

get_openstack_images_per_image_timeout_minutes: 20

# disable wireguard by default
calico_wireguard_enabled: false

monitoring:
  thanos_receive_url: https://{{ .Values.external_hostnames.thanos_receive }}:443/api/v1/receive  # for workload clusters, this is overridden via shared_workload_clusters_values to point to the mgmt cluster Thanos

logging:
  loki_url: "https://{{ .Values.external_hostnames.loki }}"  # for workload clusters, this is overridden via shared_workload_clusters_values to point to the mgmt cluster Loki

_internal:
  ha_cluster:

    # immutable ha_cluster.mode : keep track of first install mode HA or non-HA
    mode: '{{ lookup "v1" "Secret" .Release.Namespace "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | default dict | dig "_internal" "ha_cluster" "mode" (ge (int .Values.cluster.control_plane_replicas) 3  | ternary "ha" "non-ha") }}'

    # variable just used to trigger error while requested control_plane_replicas is incompatible with ha_cluster.mode computed during first install
    checkpoint: >-
      {{- $current_config := lookup "v1" "Secret" .Release.Namespace "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | default dict -}}
      {{- if dig "_internal" "ha_cluster" "mode" "" $current_config -}}
        {{- if eq $current_config._internal.ha_cluster.mode "ha" -}}
          {{- gt (int .Values.cluster.control_plane_replicas) 2  | ternary "ok" "" | required (print "requested control_plane_replicas=" .Values.cluster.control_plane_replicas ": can't continue, current mode is HA, requires a value >=3") -}}
        {{- else -}}
          {{- eq (int .Values.cluster.control_plane_replicas) 1  | ternary "ok" "" | required (print "requested control_plane_replicas=" .Values.cluster.control_plane_replicas ": can't continue, first install happened in non-HA mode, this value should be 1") -}}
        {{- end -}}
      {{- else -}}
        {{- eq (int .Values.cluster.control_plane_replicas) 2  | ternary "" "install run" | required (print "requested control_plane_replicas=" .Values.cluster.control_plane_replicas ": this mode is forbidden") -}}
      {{- end -}}

  # default replica number computed from the ha_cluster.mode value
  default_replicas: '{{ tuple . .Values._internal.ha_cluster.mode | include "interpret-as-string" | eq "ha" | ternary 3 1 | include "preserve-type" }}'

  # minio bucket root password
  minio_monitoring_root_password: '{{ lookup "v1" "Secret" .Release.Namespace "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | dig "_internal" "minio_monitoring_root_password" (randAlphaNum 64) }}'

  # minio console password
  minio_monitoring_user_password: '{{ lookup "v1" "Secret" .Release.Namespace "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | dig "_internal" "minio_monitoring_user_password" (randAlphaNum 64) }}'

  # thanos password
  thanos_password: '{{ lookup "v1" "Secret" .Release.Namespace "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | dig "_internal" "thanos_password" (randAlphaNum 64) }}'

  # loki password
  loki_password: '{{ lookup "v1" "Secret" .Release.Namespace "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | dig "_internal" "loki_password" (randAlphaNum 64) }}'

  # unless the default password has already been generated by a previous run, we generate a new one randomly
  default_password: '{{ lookup "v1" "Secret" .Release.Namespace "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | dig "_internal" "default_password" (randAlphaNum 64) }}'

  default_storage_class: >-
    {{- if (tuple . "cinder-csi" | include "unit-enabled") -}}
      {{ .Values.openstack.storageClass.name }}
    {{- else if (tuple . "vsphere-csi-driver" | include "unit-enabled") -}}
      vsphere-csi
    {{- else if (tuple . "longhorn" | include "unit-enabled") -}}
      longhorn
    {{- else -}}
      local-path
    {{- end -}}

  # maps storage class names to the name of the unit that implements it
  # (a storage class does not need to appear in this map if the required unit has the same name)
  storage_class_unit_map:
    local-path: local-path-provisioner
    longhorn: longhorn
    vsphere-csi: vsphere-csi-driver
    '{{ .Values.openstack.storageClass.name }}': cinder-csi

  default_storage_class_unit: >-
    {{- $default_storage_class := tuple . .Values._internal.default_storage_class | include "interpret-as-string" -}}
    {{- index (tuple . .Values._internal.storage_class_unit_map | include "interpret-inner-gotpl" | fromJson) "result" | dig $default_storage_class (printf "storage class unit not found for %s" $default_storage_class) -}}

  storage_class_RWX_support:
    - longhorn
  default_storage_class_RWX_support: >-
    {{- $default_storage_class := tuple . .Values._internal.default_storage_class | include "interpret-as-string" -}}
    {{- has $default_storage_class .Values._internal.storage_class_RWX_support -}}

  vault_replicas: >-
    {{ or (.Values.cluster.capi_providers.infra_provider | eq "capd") (and (lt (int .Values.cluster.control_plane_replicas) 3) (eq .Values._internal.default_storage_class_unit "local-path")) | ternary 1 3  }}

  vault_affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: vault
              vault_cr: vault
          topologyKey: kubernetes.io/hostname

  vault_no_affinity: {}

  # this is used in use-oci-artifacts.values.yaml
  # and in a few other places
  sylva_core_version: '{{ .Chart.Version }}'

  sylva_core_existing_source: '{{ .Values.unit_helmrelease_kustomization_spec_default.sourceRef }}'

  # We need to retrieve bootstrap_node_ip in management-cluster while using libvirt-metal emulation, it is stored in a ConfigMap by libvirt-metal unit in that case
  bootstrap_node_ip: '{{ lookup "v1" "ConfigMap" "sylva-system" "cluster-public-endpoint" | dig "data" "address" "not_found" | default "not_found" }}'

  metal3_unit: '{{ .Values.metal3.provider | eq "suse" | ternary "metal3-suse" "metal3" }}'

  k8s_post_1_26_6: '{{ semverCompare ">1.26.6" (tuple . .Values.cluster.k8s_version | include "interpret-as-string") | include "preserve-type" }}'

  k8s_version_map:
    1.25: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "v1.25.14+rke2r1" "v1.25.14" }}'
    1.26: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "v1.26.9+rke2r1" "v1.26.9" }}'
    1.27: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "v1.27.6+rke2r1" "v1.27.6" }}'

  # total number of nodes in this cluster
  node_count:
    '
    {{- $n := .Values.cluster.control_plane_replicas -}}
    {{- range $_, $md := .Values.cluster.machine_deployments -}}
      {{- $n = add $n $md.replicas -}}
    {{- end -}}
    {{ $n | include "preserve-type" }}
    '

  os_images_info_input_hash:
    '{{
    list
      .Values._internal.sylva_core_version
      (include "generate-os-images" . | indent 4)
      .Values._internal.os_images_info_configmap
      .Values.proxies
      .Values.no_proxy_additional
      .Values.oci_registry_insecure
    | toJson | sha256sum | trunc 8
    }}'
  default_os_images_info_configmap: os-images-info-{{ tuple . .Values._internal.os_images_info_input_hash | include "interpret-as-string" }}
  os_images_info_configmap: '{{ .Values._internal.default_os_images_info_configmap }}'  # overridden in workload-cluster.values.yaml

shared_workload_clusters_values:
  ntp: '{{ .Values.ntp | include "set-if-defined" }}'
  proxies: '{{ .Values.proxies | include "set-if-defined" }}'
  registry_mirrors: '{{ .Values.registry_mirrors | include "set-if-defined" }}'

  sylva_base_oci_registry: '{{ .Values.sylva_base_oci_registry }}'
  oci_registry_extra_ca_certs: '{{ .Values.oci_registry_extra_ca_certs | include "set-if-defined" }}'
  oci_registry_insecure: '{{ .Values.oci_registry_insecure | include "set-if-defined" }}'

  cluster:
    air_gapped: '{{ .Values.air_gapped | include "set-if-defined" }}'
    capi_providers: '{{ .Values.cluster.capi_providers | include "preserve-type" }}'
    mgmt_cluster_ip: '{{ .Values.display_external_ip }}'
    rancher_domain: '{{ .Values.cluster_domain }}'
    use_custom_rancher_dns_resolver: true
    capm3:
      image_provisioning_host: '{{ .Values.display_external_ip }}'
    capo:
      image_name: '{{ .Values.cluster.capo.image_name | include "set-if-defined" }}'
      image_key: '{{ .Values.cluster.capo.image_key | include "set-if-defined" }}'
      network_id: '{{ .Values.cluster.capo.network_id | include "set-if-defined" }}'
    default_node_class: '{{ .Values.cluster.default_node_class }}'
    node_classes: '{{ .Values.cluster.node_classes | include "preserve-type" }}'

  units:
    cluster-import:
      enabled: '{{ tuple . "rancher" | include "unit-enabled" }}'
  metal3:
    provider: '{{ .Values.metal3.provider }}'
  sylva_diskimagebuilder_version: '{{ .Values.sylva_diskimagebuilder_version }}'
  sylva_diskimagebuilder_images: '{{ .Values.sylva_diskimagebuilder_images | include "preserve-type" }}'
  os_images: '{{ .Values.os_images | include "preserve-type" }}'
  monitoring:
    thanos_receive_url: '{{ .Values.monitoring.thanos_receive_url }}'
  logging:
    loki_url: '{{ .Values.logging.loki_url }}'

shared_workload_clusters_secret_values:
  cluster:
    capo: '{{ tuple (dict "clouds_yaml" (.Values.cluster | dig "capo" "clouds_yaml" dict)) (.Values.cluster | dig "capo" "clouds_yaml" dict) | include "set-only-if" }}'
  _internal:
    thanos_password: '{{ .Values._internal.thanos_password }}'
    loki_password: '{{ .Values._internal.loki_password }}'

snmp:
  devices: []
  groups: {}

# Sample snmp configuration as it needs to be added in the secrets.yaml file
# snmp:
#   devices:
#     - name: server1
#       ip: 1.2.3.4
#       group: dell1
#     - name: server2
#       ip: 5.6.7.8
#       group: hp1
#   groups:
#     dell1:
#       version: 3
#       auth:
#         community: public
#         security_level: authPriv
#         username: snmp
#         password: xxxxx
#         auth_protocol: SHA256
#         priv_protocol: AES
#         priv_password: xxxxx
#       hw_type: dell
#     hp1:
#       version: 3
#       auth:
#         community: public
#         security_level: authPriv
#         username: snmp
#         password: xxxxx
#         auth_protocol: SHA256
#         priv_protocol: AES
#         priv_password: xxxxx
#       hw_type: hp
