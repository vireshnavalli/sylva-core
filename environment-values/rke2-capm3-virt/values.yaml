# This example environment values files will let you build
# a Sylva two-node (1 control-plane + 1 worker) mgmt baremetal cluster

cluster_domain: '{{ .Values.cluster.cluster_api_cert_extra_SANs | first }}.nip.io'

cluster_virtual_ip: 192.168.100.2

cluster:
  capi_providers:
    infra_provider: capm3
    bootstrap_provider: cabpr

  control_plane_replicas: 1

  # Add kind container address to cluster kubernetes api certificate SubjectAltNames,
  # in order to be able to reach it from the outside through kubernetes-external service created by libvirt-metal unit
  # once pivot to management cluster will happen, this IP will be retrieved in a cluster-public-endpoint configMap created for that purpose
  cluster_api_cert_extra_SANs:
    - '{{ .Values._internal.bootstrap_node_ip }}'

  rke2:
    additionalUserData:
      config:
        #cloud-config
        users:
          - name: sylva-user
            groups: users
            sudo: ALL=(ALL) NOPASSWD:ALL
            shell: /bin/bash
            lock_passwd: false
            passwd: $1$pdWEicWs$9ZWQf5.CWyXccmP8Chuu01 # sylva

  capm3:
    image_key: ubuntu-jammy-plain-rke2-1-27-6
    primary_pool_network: 192.168.100.0
    primary_pool_gateway: 192.168.100.1
    primary_pool_start: 192.168.100.20
    primary_pool_end: 192.168.100.50
    primary_pool_prefix: "24"
    provisioning_pool_network: 192.168.10.0
    provisioning_pool_gateway: 192.168.10.1
    provisioning_pool_start: 192.168.10.20
    provisioning_pool_end: 192.168.10.50
    provisioning_pool_prefix: "24"

    dns_servers: [1.1.1.1]

  control_plane:
    capm3:
      hostSelector:
        matchLabels:
          cluster-role: control-plane

      provisioning_pool_interface: ens4
      primary_pool_interface: ens5

    network_interfaces:
      ens4:
        type: phy
      ens5:
        type: phy

  baremetal_host_default:
    bmh_metadata:
      annotations:
        sylvaproject.org/default-longhorn-disks-config: >-
          [{"path":"/var/longhorn/disks/disk_by-path_pci-0000:00:0a.0", "storageReserved":0, "allowScheduling":true, "tags":[ "vhdd", "fast"] },
           {"path":"/var/longhorn/disks/disk_by-path_pci-0000:00:0b.0", "storageReserved":0, "allowScheduling":true, "tags":[ "vhdd", "fast"] }]
    bmh_spec:
      online: false
      description: management cluster node
      bmc:
        disableCertificateVerification: true
      bootMode: legacy
      automatedCleaningMode: disabled
      rootDeviceHints:
        deviceName: /dev/vda

  baremetal_hosts:
    management-cp-0:
      bmh_metadata:
        labels:
          cluster-role: control-plane
      bmh_spec:
        bmc:
          address: redfish-virtualmedia://{{ .Values._internal.bootstrap_node_ip }}:8000/redfish/v1/Systems/c0014001-b10b-f001-c0de-feeb1e54ee15
        bootMACAddress: 52:54:00:44:44:00


units:

  longhorn:
    # this setting can be overridden to true to enable Longhorn
    enabled: '{{ .Values.env_type | eq "ci" }}'
    helmrelease_spec:
      values:
        persistence:
          defaultClassReplicaCount: 1   # don't enable data replication (this is a single node setup)

  minio-monitoring-tenant:
    enabled_conditions:
      - '{{ ternary true false (.Values.env_type | eq "ci") | include "preserve-type" }}'

  ingress-nginx:
    helmrelease_spec:
      values:
        controller:
          image:
            tag: nginx-1.8.1-rancher1  # fix for https://gitlab.com/sylva-projects/sylva-core/-/issues/696

libvirt_metal:
  #image: registry.gitlab.com/sylva-projects/sylva-elements/container-images/libvirt-metal:0.1.4  # customize if wanted
  nodes:
    management-cp-0:
      redfishPort: 8000
      memGB: 16
      numCPUs: 8

    workload-cp-0:
      redfishPort: 8010
      memGB: 6 # IPA allocates half of it to tmpfs, if must be large enough to store the image.
      numCPUs: 4

# dummy configuration to allow deployment of snmp-exporter
snmp:
  devices:
    - name: bar
      ip: 192.168.100.2
      group: foo
    - name: abc
      ip: 192.168.100.2
      group: def
  groups:
    foo:
      version: 3
      auth:
        community: public
        security_level: authPriv
        username: user
        password: xxxxxx
        auth_protocol: SHA256
        priv_protocol: AES
        priv_password: xxxxxx
      hw_type: dell
    def:
      version: 3
      auth:
        community: public
        security_level: authPriv
        username: user
        password: xxxxxx
        auth_protocol: SHA256
        priv_protocol: AES
        priv_password: xxxxxx
      hw_type: hp


#proxies:
#  # put your own proxy settings here if you need
#  http_proxy: ""
#  https_proxy: ""
#  no_proxy: ""

# configure containerd registry mirrors following https://github.com/containerd/containerd/blob/main/docs/hosts.md
# see charts/syla-units/values.yaml for a more detailled example
# registry_mirrors:
#   hosts_config:
#     docker.io:
#     - mirror_url: http://your.mirror/docker
